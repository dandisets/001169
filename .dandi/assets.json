[
{
"asset_id":
"af45a451-33de-4c23-a241-e9779e168783",
"blob":
"c24b99d5-5411-4331-863d-50b23f4b32fe",
"created":
"2024-09-13T12:41:25.883324Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:38:49.896790+02:00",
"contentSize":
714186,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/af45a451-33de-4c23-a241-e9779e168783/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/c24/b99/c24b99d5-5411-4331-863d-50b23f4b32fe"
],
"dateModified":
"2024-09-13T14:41:21.047567+02:00",
"digest":
{
"dandi:dandi-etag":
"18be19bde24c9abbff6d68e0aa74ae3e-1",
"dandi:sha2-256":
"bb7e144b41641218a1ea077ff39dbcc3b6a40b8cf1751b0682f0fd9695025c0a"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:af45a451-33de-4c23-a241-e9779e168783",
"identifier":
"af45a451-33de-4c23-a241-e9779e168783",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait-20191017-093757.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait-20191017-093757",
"name":
"RWTautowait-20191017-093757",
"schemaKey":
"Session",
"startDate":
"2019-10-17T09:41:43-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:21.047557+02:00",
"id":
"urn:uuid:354466de-108c-4c04-a7a9-a6ce84d1b8e5",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:19.908559+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:25.883343Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait-20191017-093757.nwb",
"size":
714186
},
{
"asset_id":
"2aaa45a9-fc71-4699-9a99-f070e0fc6ace",
"blob":
"01a112a2-e8b2-4d3d-80d6-b7ecd4c5a949",
"created":
"2024-09-13T12:41:23.832845Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:38:52.608726+02:00",
"contentSize":
728300,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/2aaa45a9-fc71-4699-9a99-f070e0fc6ace/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/01a/112/01a112a2-e8b2-4d3d-80d6-b7ecd4c5a949"
],
"dateModified":
"2024-09-13T14:41:20.011344+02:00",
"digest":
{
"dandi:dandi-etag":
"08f9e56b5eb1639166685d8d4c51c697-1",
"dandi:sha2-256":
"44505fa5d0bc7fe40bc9a1cec4404e46fd7320e0e3de31db036c813b601a8498"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:2aaa45a9-fc71-4699-9a99-f070e0fc6ace",
"identifier":
"2aaa45a9-fc71-4699-9a99-f070e0fc6ace",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait-20191030-100319.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait-20191030-100319",
"name":
"RWTautowait-20191030-100319",
"schemaKey":
"Session",
"startDate":
"2019-10-30T10:06:39-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:20.011332+02:00",
"id":
"urn:uuid:84047ddb-fbf0-47fe-84c6-865a09dbc2a4",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:19.632762+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:23.832865Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait-20191030-100319.nwb",
"size":
728300
},
{
"asset_id":
"ae339c73-1309-47bd-bfb5-935846473321",
"blob":
"f518caea-1a76-416e-9c8f-760dc795fa95",
"created":
"2024-09-13T12:41:24.519305Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:38:54.924879+02:00",
"contentSize":
669592,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/ae339c73-1309-47bd-bfb5-935846473321/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/f51/8ca/f518caea-1a76-416e-9c8f-760dc795fa95"
],
"dateModified":
"2024-09-13T14:41:21.158718+02:00",
"digest":
{
"dandi:dandi-etag":
"7da5d2c4ea082267af67bcd3043b57ab-1",
"dandi:sha2-256":
"67915315465d785a3623805cbc12bf7aca0542380eaf9f7052497350e39ee0f0"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:ae339c73-1309-47bd-bfb5-935846473321",
"identifier":
"ae339c73-1309-47bd-bfb5-935846473321",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait-20191031-102311.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait-20191031-102311",
"name":
"RWTautowait-20191031-102311",
"schemaKey":
"Session",
"startDate":
"2019-10-31T10:25:27-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:21.158710+02:00",
"id":
"urn:uuid:564219be-df50-40e4-8768-88e82fe08341",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:19.982482+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:24.519324Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait-20191031-102311.nwb",
"size":
669592
},
{
"asset_id":
"0e0c3fe1-8c23-4294-8e1e-2b4e11d592ec",
"blob":
"2ec240c1-3ee1-4203-b02c-78b32ae4d023",
"created":
"2024-09-13T12:41:24.405905Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:38:57.250241+02:00",
"contentSize":
643999,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/0e0c3fe1-8c23-4294-8e1e-2b4e11d592ec/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/2ec/240/2ec240c1-3ee1-4203-b02c-78b32ae4d023"
],
"dateModified":
"2024-09-13T14:41:21.069634+02:00",
"digest":
{
"dandi:dandi-etag":
"37ea3b9030e4436f5397298394da214a-1",
"dandi:sha2-256":
"7755c8b8c472ee4595266e0b507d330d3481fcf15159100ff190690b7f10c953"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:0e0c3fe1-8c23-4294-8e1e-2b4e11d592ec",
"identifier":
"0e0c3fe1-8c23-4294-8e1e-2b4e11d592ec",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait-20191106-100756.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait-20191106-100756",
"name":
"RWTautowait-20191106-100756",
"schemaKey":
"Session",
"startDate":
"2019-11-06T10:14:10-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:21.069624+02:00",
"id":
"urn:uuid:a1facdbb-a1f0-4955-87cc-a95a16aadbd0",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:19.889443+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:24.405925Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait-20191106-100756.nwb",
"size":
643999
},
{
"asset_id":
"2f955236-7e2b-45db-b6d7-af04229f86be",
"blob":
"88de2a8f-2035-49db-81dc-9d53dcb0e3af",
"created":
"2024-09-13T12:41:24.995468Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:38:59.952772+02:00",
"contentSize":
695203,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/2f955236-7e2b-45db-b6d7-af04229f86be/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/88d/e2a/88de2a8f-2035-49db-81dc-9d53dcb0e3af"
],
"dateModified":
"2024-09-13T14:41:21.132963+02:00",
"digest":
{
"dandi:dandi-etag":
"ec3e0e90c35ee138421fddd3bfbcbff7-1",
"dandi:sha2-256":
"541baa05b11226bad1b3297eb2b4161da6d84e8260826462eae7f61f19c72fff"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:2f955236-7e2b-45db-b6d7-af04229f86be",
"identifier":
"2f955236-7e2b-45db-b6d7-af04229f86be",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait-20191107-092944.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait-20191107-092944",
"name":
"RWTautowait-20191107-092944",
"schemaKey":
"Session",
"startDate":
"2019-11-07T09:35:44-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:21.132954+02:00",
"id":
"urn:uuid:ec8842da-1db2-47dd-b6fe-a467d28b7e08",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:19.917440+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:24.995487Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait-20191107-092944.nwb",
"size":
695203
},
{
"asset_id":
"6e4c27cb-4cb2-4f6a-a3ae-61e1082d1048",
"blob":
"a0bd2937-c41f-4dec-9bd0-c082c172304d",
"created":
"2024-09-13T12:41:27.945455Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:02.556899+02:00",
"contentSize":
687082,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/6e4c27cb-4cb2-4f6a-a3ae-61e1082d1048/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/a0b/d29/a0bd2937-c41f-4dec-9bd0-c082c172304d"
],
"dateModified":
"2024-09-13T14:41:25.703442+02:00",
"digest":
{
"dandi:dandi-etag":
"0b5a3449dd843a4e776be18ae76366b1-1",
"dandi:sha2-256":
"fd015b8c29b66ff1f39c8590392796e888e6f222c04e2a6dc64cdf1af6cad340"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:6e4c27cb-4cb2-4f6a-a3ae-61e1082d1048",
"identifier":
"6e4c27cb-4cb2-4f6a-a3ae-61e1082d1048",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait-20191111-100147.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait-20191111-100147",
"name":
"RWTautowait-20191111-100147",
"schemaKey":
"Session",
"startDate":
"2019-11-11T10:02:58-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:25.703430+02:00",
"id":
"urn:uuid:63806dfb-2872-46c2-a2af-b8411a553974",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:24.935264+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:27.945479Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait-20191111-100147.nwb",
"size":
687082
},
{
"asset_id":
"e3840b1d-c97b-46e1-8255-60c86578090d",
"blob":
"5589f5bf-cf7d-4190-b931-8973d4567769",
"created":
"2024-09-13T12:41:30.979900Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:05.079640+02:00",
"contentSize":
673028,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/e3840b1d-c97b-46e1-8255-60c86578090d/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/558/9f5/5589f5bf-cf7d-4190-b931-8973d4567769"
],
"dateModified":
"2024-09-13T14:41:28.610206+02:00",
"digest":
{
"dandi:dandi-etag":
"47a604b784df189d4259e3e6bfa13026-1",
"dandi:sha2-256":
"3306ef15e652ee4a609880cb2fae283455a6ad32908e7c1df227cdc19844f469"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:e3840b1d-c97b-46e1-8255-60c86578090d",
"identifier":
"e3840b1d-c97b-46e1-8255-60c86578090d",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191112-092235.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20191112-092235",
"name":
"RWTautowait2-20191112-092235",
"schemaKey":
"Session",
"startDate":
"2019-11-12T09:23:23-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:28.610196+02:00",
"id":
"urn:uuid:ba99e103-223c-4f7e-80b2-1f6a3f07e66b",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:26.871201+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:30.979923Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191112-092235.nwb",
"size":
673028
},
{
"asset_id":
"e6df8dd9-f989-4f32-9c29-bc0bfde130b4",
"blob":
"f023df85-90e8-4b69-b7ea-a3d4ef621cd0",
"created":
"2024-09-13T12:41:30.415189Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:07.588796+02:00",
"contentSize":
684189,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/e6df8dd9-f989-4f32-9c29-bc0bfde130b4/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/f02/3df/f023df85-90e8-4b69-b7ea-a3d4ef621cd0"
],
"dateModified":
"2024-09-13T14:41:28.116353+02:00",
"digest":
{
"dandi:dandi-etag":
"87189cc829d77555a9217cf961e1c343-1",
"dandi:sha2-256":
"dd28b9e1b401f2644f137d0769b0001b145d2455de3e2d9f34a35b0f130037aa"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:e6df8dd9-f989-4f32-9c29-bc0bfde130b4",
"identifier":
"e6df8dd9-f989-4f32-9c29-bc0bfde130b4",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191119-093820.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20191119-093820",
"name":
"RWTautowait2-20191119-093820",
"schemaKey":
"Session",
"startDate":
"2019-11-19T09:42:33-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:28.116342+02:00",
"id":
"urn:uuid:ac5ac80f-4c04-4664-95e3-3aff2eb8c918",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:27.135760+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:30.415212Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191119-093820.nwb",
"size":
684189
},
{
"asset_id":
"3f758669-9d0f-4a44-a3a6-58551f3cddfd",
"blob":
"efbca45c-7605-4230-b712-06aa3c350e31",
"created":
"2024-09-13T12:41:30.868261Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:10.586143+02:00",
"contentSize":
770480,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/3f758669-9d0f-4a44-a3a6-58551f3cddfd/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/efb/ca4/efbca45c-7605-4230-b712-06aa3c350e31"
],
"dateModified":
"2024-09-13T14:41:28.298089+02:00",
"digest":
{
"dandi:dandi-etag":
"3c569bf62553e7a4be12120fde2647e8-1",
"dandi:sha2-256":
"a78e9f207391be8f9b5c4d330b17e72121318285ee0734004b866a602075c82e"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:3f758669-9d0f-4a44-a3a6-58551f3cddfd",
"identifier":
"3f758669-9d0f-4a44-a3a6-58551f3cddfd",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191120-094043.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20191120-094043",
"name":
"RWTautowait2-20191120-094043",
"schemaKey":
"Session",
"startDate":
"2019-11-20T09:46:37-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:28.298077+02:00",
"id":
"urn:uuid:78e9b610-e1cf-43f8-91ea-5c1ed0ca1661",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:27.266664+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:30.868276Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191120-094043.nwb",
"size":
770480
},
{
"asset_id":
"e73820ca-98dd-4ca5-bea0-f9734950159d",
"blob":
"a8444d78-1fed-4acc-b971-acd3cb460e5b",
"created":
"2024-09-13T12:41:32.006876Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:13.281297+02:00",
"contentSize":
709169,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/e73820ca-98dd-4ca5-bea0-f9734950159d/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/a84/44d/a8444d78-1fed-4acc-b971-acd3cb460e5b"
],
"dateModified":
"2024-09-13T14:41:29.009627+02:00",
"digest":
{
"dandi:dandi-etag":
"ffc2bf9d163e828afcfea12aaceab0d1-1",
"dandi:sha2-256":
"1a8cf40d134d520064b0c1a1eec2ff8149c242f9f888085ee2d54d9e312e8c1d"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:e73820ca-98dd-4ca5-bea0-f9734950159d",
"identifier":
"e73820ca-98dd-4ca5-bea0-f9734950159d",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191121-095804.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20191121-095804",
"name":
"RWTautowait2-20191121-095804",
"schemaKey":
"Session",
"startDate":
"2019-11-21T10:03:36-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:29.009618+02:00",
"id":
"urn:uuid:37b6c503-22bf-4c67-9046-90b23b127d8d",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:27.904389+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:32.006900Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191121-095804.nwb",
"size":
709169
},
{
"asset_id":
"3342e15e-b984-46ce-aecb-6855c47d52cf",
"blob":
"5d206cde-2193-4524-9bc5-2bf7bf8972c3",
"created":
"2024-09-13T12:41:32.335072Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:15.692672+02:00",
"contentSize":
655452,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/3342e15e-b984-46ce-aecb-6855c47d52cf/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/5d2/06c/5d206cde-2193-4524-9bc5-2bf7bf8972c3"
],
"dateModified":
"2024-09-13T14:41:29.723380+02:00",
"digest":
{
"dandi:dandi-etag":
"48a80c6689e53b89da40d0c44f7e33ac-1",
"dandi:sha2-256":
"76aa268f6059e3c9e0545de9ddb0aa189d38af918b5bf30c8febc8198e26280b"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:3342e15e-b984-46ce-aecb-6855c47d52cf",
"identifier":
"3342e15e-b984-46ce-aecb-6855c47d52cf",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191125-093420.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20191125-093420",
"name":
"RWTautowait2-20191125-093420",
"schemaKey":
"Session",
"startDate":
"2019-11-25T09:38:04-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:29.723369+02:00",
"id":
"urn:uuid:66a7dc21-4fd8-432d-b5b9-5d631a08b63e",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:29.383615+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:32.335091Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191125-093420.nwb",
"size":
655452
},
{
"asset_id":
"4618dde6-9fea-492a-b75f-fea22617f45f",
"blob":
"3684638d-4da5-4f8e-b45d-f171dec46e97",
"created":
"2024-09-13T12:41:34.917531Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:18.063608+02:00",
"contentSize":
639763,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/4618dde6-9fea-492a-b75f-fea22617f45f/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/368/463/3684638d-4da5-4f8e-b45d-f171dec46e97"
],
"dateModified":
"2024-09-13T14:41:32.352725+02:00",
"digest":
{
"dandi:dandi-etag":
"941dbe2bd61d713176d15016858f1c69-1",
"dandi:sha2-256":
"a25c73449656f2ea09298d9cd254519ccb2a5ad62e70e14a71c4caddf75bad36"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:4618dde6-9fea-492a-b75f-fea22617f45f",
"identifier":
"4618dde6-9fea-492a-b75f-fea22617f45f",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191206-100532.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20191206-100532",
"name":
"RWTautowait2-20191206-100532",
"schemaKey":
"Session",
"startDate":
"2019-12-06T10:05:45-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:32.352713+02:00",
"id":
"urn:uuid:d19b45f2-e127-43f6-98fb-e7de8f360b73",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:31.573901+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:34.917550Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191206-100532.nwb",
"size":
639763
},
{
"asset_id":
"422ae432-9517-434c-917d-b170193dfba9",
"blob":
"892721af-ceca-4c2d-b8f3-fbe1caaa5b4c",
"created":
"2024-09-13T12:41:36.639854Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:20.578737+02:00",
"contentSize":
689617,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/422ae432-9517-434c-917d-b170193dfba9/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/892/721/892721af-ceca-4c2d-b8f3-fbe1caaa5b4c"
],
"dateModified":
"2024-09-13T14:41:34.162693+02:00",
"digest":
{
"dandi:dandi-etag":
"22bb1df80b796933a7fa27a2e82874ba-1",
"dandi:sha2-256":
"16d103385355eee1f9243512c868ffa5dbd9c45bcea9302366b40fd90a2fb5b0"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:422ae432-9517-434c-917d-b170193dfba9",
"identifier":
"422ae432-9517-434c-917d-b170193dfba9",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191209-102816.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20191209-102816",
"name":
"RWTautowait2-20191209-102816",
"schemaKey":
"Session",
"startDate":
"2019-12-09T10:29:19-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:34.162682+02:00",
"id":
"urn:uuid:47a05c06-29dc-4c00-a041-9b2d06780525",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:33.103693+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:36.639872Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191209-102816.nwb",
"size":
689617
},
{
"asset_id":
"fe672f6c-4493-47a2-9fae-99720ddf5acf",
"blob":
"46051b30-dee2-4e64-b617-e122b9bc3b06",
"created":
"2024-09-13T12:41:36.754063Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:23.467036+02:00",
"contentSize":
757213,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/fe672f6c-4493-47a2-9fae-99720ddf5acf/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/460/51b/46051b30-dee2-4e64-b617-e122b9bc3b06"
],
"dateModified":
"2024-09-13T14:41:34.194942+02:00",
"digest":
{
"dandi:dandi-etag":
"48054c4481ad1536d2496255177cb263-1",
"dandi:sha2-256":
"e39139b57af6239e92c1e3f37fa5a04e9ca156dc12fcb8b6fd19c22f44d6c894"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:fe672f6c-4493-47a2-9fae-99720ddf5acf",
"identifier":
"fe672f6c-4493-47a2-9fae-99720ddf5acf",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191212-095540.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20191212-095540",
"name":
"RWTautowait2-20191212-095540",
"schemaKey":
"Session",
"startDate":
"2019-12-12T09:58:20-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:34.194933+02:00",
"id":
"urn:uuid:48bcdce1-edf3-41f5-88cd-546abd1461c6",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:33.147637+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:36.754083Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191212-095540.nwb",
"size":
757213
},
{
"asset_id":
"2004d863-8f97-4d0a-bc64-73ae11cd239f",
"blob":
"7555fdcb-db24-4737-a10e-c3c91e5c6ef2",
"created":
"2024-09-13T12:41:37.916047Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:26.865917+02:00",
"contentSize":
830151,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/2004d863-8f97-4d0a-bc64-73ae11cd239f/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/755/5fd/7555fdcb-db24-4737-a10e-c3c91e5c6ef2"
],
"dateModified":
"2024-09-13T14:41:35.271618+02:00",
"digest":
{
"dandi:dandi-etag":
"f90a40728e0297a7875e5865c7064bb1-1",
"dandi:sha2-256":
"660102837c33f449e409c8f8b41c57582a249bce2659099db5d76ddf5a15e2d6"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:2004d863-8f97-4d0a-bc64-73ae11cd239f",
"identifier":
"2004d863-8f97-4d0a-bc64-73ae11cd239f",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191213-101246.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20191213-101246",
"name":
"RWTautowait2-20191213-101246",
"schemaKey":
"Session",
"startDate":
"2019-12-13T10:13:05-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:35.271608+02:00",
"id":
"urn:uuid:8c19d18b-d14b-411f-8751-04dc3bd7c382",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:34.688745+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:37.916062Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191213-101246.nwb",
"size":
830151
},
{
"asset_id":
"aceaa901-2421-4cc6-88d5-b46fa0ac2408",
"blob":
"2959507f-478f-473e-ae2e-5620bc155a33",
"created":
"2024-09-13T12:41:40.597619Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:30.241073+02:00",
"contentSize":
832736,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/aceaa901-2421-4cc6-88d5-b46fa0ac2408/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/295/950/2959507f-478f-473e-ae2e-5620bc155a33"
],
"dateModified":
"2024-09-13T14:41:34.803604+02:00",
"digest":
{
"dandi:dandi-etag":
"62bc4ddcc2d09b6d0852ef6288e2da2e-1",
"dandi:sha2-256":
"886a95fed2012b89ed334375bd7af21717e51b87663c1538bb470a9fc60f72cd"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:aceaa901-2421-4cc6-88d5-b46fa0ac2408",
"identifier":
"aceaa901-2421-4cc6-88d5-b46fa0ac2408",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191216-100126.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20191216-100126",
"name":
"RWTautowait2-20191216-100126",
"schemaKey":
"Session",
"startDate":
"2019-12-16T10:02:01-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:34.803590+02:00",
"id":
"urn:uuid:4526c797-0e61-4c0d-aa32-9e4eb6a06723",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:34.362219+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:40.597638Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191216-100126.nwb",
"size":
832736
},
{
"asset_id":
"9155c514-bf87-40b7-8a5d-e96cf40ce0b5",
"blob":
"65cd706c-8178-4522-b895-ca308597cea0",
"created":
"2024-09-13T12:41:38.525822Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:33.311730+02:00",
"contentSize":
784568,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/9155c514-bf87-40b7-8a5d-e96cf40ce0b5/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/65c/d70/65cd706c-8178-4522-b895-ca308597cea0"
],
"dateModified":
"2024-09-13T14:41:36.136214+02:00",
"digest":
{
"dandi:dandi-etag":
"17bcac44c30dd664feb0405af2375c93-1",
"dandi:sha2-256":
"bddce3cfe876e9ecebca2a34d64c185a618a90083886574033b9349406dc94a0"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:9155c514-bf87-40b7-8a5d-e96cf40ce0b5",
"identifier":
"9155c514-bf87-40b7-8a5d-e96cf40ce0b5",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191217-100526.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20191217-100526",
"name":
"RWTautowait2-20191217-100526",
"schemaKey":
"Session",
"startDate":
"2019-12-17T10:05:34-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:36.136204+02:00",
"id":
"urn:uuid:9aeaff4d-7dc1-4ded-b6e9-04f42835c9ee",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:35.852378+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:38.525841Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191217-100526.nwb",
"size":
784568
},
{
"asset_id":
"f7c1a9fd-561d-49c3-aaa4-a466dfc7dabf",
"blob":
"6dface7e-8723-4323-9554-1abc72ac6e48",
"created":
"2024-09-13T12:41:41.542966Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:36.583949+02:00",
"contentSize":
816675,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/f7c1a9fd-561d-49c3-aaa4-a466dfc7dabf/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/6df/ace/6dface7e-8723-4323-9554-1abc72ac6e48"
],
"dateModified":
"2024-09-13T14:41:39.061668+02:00",
"digest":
{
"dandi:dandi-etag":
"d82784a911195cd3160cdbc324c64151-1",
"dandi:sha2-256":
"08f64992b53e4e440eb8c43649c3819555a1a191cd2cd13522a50a7439ebfdb8"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:f7c1a9fd-561d-49c3-aaa4-a466dfc7dabf",
"identifier":
"f7c1a9fd-561d-49c3-aaa4-a466dfc7dabf",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191218-085205.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20191218-085205",
"name":
"RWTautowait2-20191218-085205",
"schemaKey":
"Session",
"startDate":
"2019-12-18T08:52:33-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:39.061658+02:00",
"id":
"urn:uuid:def4c897-bed8-4b8c-80f3-c5fcb0e77ff5",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:38.048330+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:41.542987Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20191218-085205.nwb",
"size":
816675
},
{
"asset_id":
"43e4a3d4-664a-4e80-a58b-506c7ee8fc35",
"blob":
"8127cbb5-a6ff-46de-b397-b159c8151601",
"created":
"2024-09-13T12:41:41.410794Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:40.586303+02:00",
"contentSize":
937898,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/43e4a3d4-664a-4e80-a58b-506c7ee8fc35/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/812/7cb/8127cbb5-a6ff-46de-b397-b159c8151601"
],
"dateModified":
"2024-09-13T14:41:39.000523+02:00",
"digest":
{
"dandi:dandi-etag":
"4c933db353f7bb2a44051a725f9bc55a-1",
"dandi:sha2-256":
"64e0a2719d29dee861ab591fb2e5da1d0dc79cc08d6062860345d07d6b0dde37"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:43e4a3d4-664a-4e80-a58b-506c7ee8fc35",
"identifier":
"43e4a3d4-664a-4e80-a58b-506c7ee8fc35",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200102-093559.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20200102-093559",
"name":
"RWTautowait2-20200102-093559",
"schemaKey":
"Session",
"startDate":
"2020-01-02T09:36:07-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:39.000513+02:00",
"id":
"urn:uuid:d824d247-2c0f-48fd-ba21-972f6e9c1e2b",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:38.103446+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:41.410809Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200102-093559.nwb",
"size":
937898
},
{
"asset_id":
"cf729c23-7700-4c95-af08-20b1ae76c055",
"blob":
"42aaab53-b6b4-4252-9c4f-e25d6016b569",
"created":
"2024-09-13T12:41:42.576600Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:43.564127+02:00",
"contentSize":
767565,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/cf729c23-7700-4c95-af08-20b1ae76c055/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/42a/aab/42aaab53-b6b4-4252-9c4f-e25d6016b569"
],
"dateModified":
"2024-09-13T14:41:40.139190+02:00",
"digest":
{
"dandi:dandi-etag":
"4c24633fe2388ba1548317cd1fab5539-1",
"dandi:sha2-256":
"e9214e0aeb9ec1da4470cf010dcf5cda1f8585e75d6b381da60b253c1026146a"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:cf729c23-7700-4c95-af08-20b1ae76c055",
"identifier":
"cf729c23-7700-4c95-af08-20b1ae76c055",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200103-093727.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20200103-093727",
"name":
"RWTautowait2-20200103-093727",
"schemaKey":
"Session",
"startDate":
"2020-01-03T09:41:39-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:40.139181+02:00",
"id":
"urn:uuid:041a9883-bb26-4f30-ad3a-9939f8242714",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:39.796301+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:42.576630Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200103-093727.nwb",
"size":
767565
},
{
"asset_id":
"2f130ced-805b-4bd5-8f56-3510d5beaa74",
"blob":
"53ddce69-209f-4216-8e30-a2f9da7efa41",
"created":
"2024-09-13T12:41:45.703553Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:46.914712+02:00",
"contentSize":
805082,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/2f130ced-805b-4bd5-8f56-3510d5beaa74/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/53d/dce/53ddce69-209f-4216-8e30-a2f9da7efa41"
],
"dateModified":
"2024-09-13T14:41:40.422491+02:00",
"digest":
{
"dandi:dandi-etag":
"3b74dc4b8ef0423006343e12a6676193-1",
"dandi:sha2-256":
"2fa22741638bfe54ffd352c82a0c0272c11a3a33b2003609194919e9871abeca"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:2f130ced-805b-4bd5-8f56-3510d5beaa74",
"identifier":
"2f130ced-805b-4bd5-8f56-3510d5beaa74",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200106-094448.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20200106-094448",
"name":
"RWTautowait2-20200106-094448",
"schemaKey":
"Session",
"startDate":
"2020-01-06T09:53:25-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:40.422482+02:00",
"id":
"urn:uuid:befbbd95-49b0-485c-933c-9d065ce34283",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:40.040827+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:45.703568Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200106-094448.nwb",
"size":
805082
},
{
"asset_id":
"a0221854-a7d5-4f22-9f5f-fe2a088b2646",
"blob":
"aaf856f8-9518-4f6c-b3ad-1f855a23b470",
"created":
"2024-09-13T12:41:44.560142Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:49.913195+02:00",
"contentSize":
761751,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/a0221854-a7d5-4f22-9f5f-fe2a088b2646/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/aaf/856/aaf856f8-9518-4f6c-b3ad-1f855a23b470"
],
"dateModified":
"2024-09-13T14:41:42.159808+02:00",
"digest":
{
"dandi:dandi-etag":
"20fb3a9e0971ccd42cbea26962d42d2d-1",
"dandi:sha2-256":
"4a2f47593c45a02ce24e611406427824f5ff9ee866067f82045da62e4865e12e"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:a0221854-a7d5-4f22-9f5f-fe2a088b2646",
"identifier":
"a0221854-a7d5-4f22-9f5f-fe2a088b2646",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200107-101256.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20200107-101256",
"name":
"RWTautowait2-20200107-101256",
"schemaKey":
"Session",
"startDate":
"2020-01-07T10:15:13-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:42.159798+02:00",
"id":
"urn:uuid:fbba646a-297d-4637-be57-aa767c8a7ada",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:41.411474+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:44.560161Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200107-101256.nwb",
"size":
761751
},
{
"asset_id":
"186388e3-ba5c-4dce-8520-7901e026c6c4",
"blob":
"5e8242ab-9462-4141-8d0a-03ee64ab4161",
"created":
"2024-09-13T12:41:46.678694Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:53.755394+02:00",
"contentSize":
881014,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/186388e3-ba5c-4dce-8520-7901e026c6c4/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/5e8/242/5e8242ab-9462-4141-8d0a-03ee64ab4161"
],
"dateModified":
"2024-09-13T14:41:44.065701+02:00",
"digest":
{
"dandi:dandi-etag":
"d585d5745e57732fa0d41c5fc275973b-1",
"dandi:sha2-256":
"de95a46ae7e79edbfcf9523dfc296106b691fb22f7eca426075a22d928e2cb7a"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:186388e3-ba5c-4dce-8520-7901e026c6c4",
"identifier":
"186388e3-ba5c-4dce-8520-7901e026c6c4",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200108-100122.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20200108-100122",
"name":
"RWTautowait2-20200108-100122",
"schemaKey":
"Session",
"startDate":
"2020-01-08T10:07:00-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:44.065689+02:00",
"id":
"urn:uuid:ad58aa85-9765-471c-9457-34e408523570",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:43.116773+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:46.678715Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200108-100122.nwb",
"size":
881014
},
{
"asset_id":
"4ecbb960-b3c8-4e5e-8294-ba9be2c3cf02",
"blob":
"53f047e4-2527-4942-a9ac-0889d29d222f",
"created":
"2024-09-13T12:41:46.671558Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:39:57.126648+02:00",
"contentSize":
817251,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/4ecbb960-b3c8-4e5e-8294-ba9be2c3cf02/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/53f/047/53f047e4-2527-4942-a9ac-0889d29d222f"
],
"dateModified":
"2024-09-13T14:41:43.997569+02:00",
"digest":
{
"dandi:dandi-etag":
"d8be4a10aeec1e15fc699f0c1e004ece-1",
"dandi:sha2-256":
"ed769f4b45048831ccf1c7dac23ad5c3a7ee7dd91d5bf30026524a76be993013"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:4ecbb960-b3c8-4e5e-8294-ba9be2c3cf02",
"identifier":
"4ecbb960-b3c8-4e5e-8294-ba9be2c3cf02",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200109-094411.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20200109-094411",
"name":
"RWTautowait2-20200109-094411",
"schemaKey":
"Session",
"startDate":
"2020-01-09T09:46:10-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:43.997559+02:00",
"id":
"urn:uuid:c01098df-ba04-4e5e-be15-8c76c3771140",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:43.083698+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:46.671576Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200109-094411.nwb",
"size":
817251
},
{
"asset_id":
"2279298a-76b8-4cf5-a7e9-59bc56c77a9a",
"blob":
"bde43550-ee05-47f4-a6a2-cd141a961f74",
"created":
"2024-09-13T12:41:47.043518Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:40:00.450264+02:00",
"contentSize":
814595,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/2279298a-76b8-4cf5-a7e9-59bc56c77a9a/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/bde/435/bde43550-ee05-47f4-a6a2-cd141a961f74"
],
"dateModified":
"2024-09-13T14:41:44.395362+02:00",
"digest":
{
"dandi:dandi-etag":
"78912dc6e8d01e6774ef2e02c0d22dc2-1",
"dandi:sha2-256":
"21f39fc1ac153db1621ef525e9ec8e83c90ad112a22f6a88b60dc3f685249322"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:2279298a-76b8-4cf5-a7e9-59bc56c77a9a",
"identifier":
"2279298a-76b8-4cf5-a7e9-59bc56c77a9a",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200110-095217.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P6M/P24M",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"C015",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000383",
"name":
"Female",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20200110-095217",
"name":
"RWTautowait2-20200110-095217",
"schemaKey":
"Session",
"startDate":
"2020-01-10T09:58:41-05:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:41:44.395353+02:00",
"id":
"urn:uuid:d5c4f5db-71c8-4e62-9500-468d6ee9ce5e",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:41:44.089363+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:41:47.043537Z",
"path":
"sub-C015/sub-C015_ses-RWTautowait2-20200110-095217.nwb",
"size":
814595
},
{
"asset_id":
"afb0abc2-902e-4d7e-8c08-e38a5aae0537",
"blob":
"a8075734-504c-4556-a580-293a2277cd6f",
"created":
"2024-09-13T12:58:26.085344Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:43:37.572621+02:00",
"contentSize":
1533293,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/afb0abc2-902e-4d7e-8c08-e38a5aae0537/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/a80/757/a8075734-504c-4556-a580-293a2277cd6f"
],
"dateModified":
"2024-09-13T14:58:23.063945+02:00",
"digest":
{
"dandi:dandi-etag":
"99f502a9a43124a8784d68ffa63245e4-1",
"dandi:sha2-256":
"6effc24cc809dbeb7ade2e5f0f29d22eac3c82999fcc84e3b7488ba08b5fa2a9"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:afb0abc2-902e-4d7e-8c08-e38a5aae0537",
"identifier":
"afb0abc2-902e-4d7e-8c08-e38a5aae0537",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230403-150010.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P83DT72068S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230403-150010",
"name":
"RWTautowait2-20230403-150010",
"schemaKey":
"Session",
"startDate":
"2023-04-03T15:01:08-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:23.063932+02:00",
"id":
"urn:uuid:245106dd-a5de-4844-8e89-319bcc0ed161",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:21.257126+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:26.107955Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230403-150010.nwb",
"size":
1533293
},
{
"asset_id":
"0d13a8b4-d4cd-4ec3-bb2e-a7d9c6ad5ff9",
"blob":
"0079922b-063e-472d-8bff-9b8ffb66fcf2",
"created":
"2024-09-13T12:58:26.748834Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:43:46.997386+02:00",
"contentSize":
1408563,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/0d13a8b4-d4cd-4ec3-bb2e-a7d9c6ad5ff9/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/007/992/0079922b-063e-472d-8bff-9b8ffb66fcf2"
],
"dateModified":
"2024-09-13T14:58:23.760957+02:00",
"digest":
{
"dandi:dandi-etag":
"e2a7bd7e220557311bfa5d81eced1599-1",
"dandi:sha2-256":
"1987516ac9fe52ff89d27df36e320adba3a7d3308d170f70961257986cd4f623"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:0d13a8b4-d4cd-4ec3-bb2e-a7d9c6ad5ff9",
"identifier":
"0d13a8b4-d4cd-4ec3-bb2e-a7d9c6ad5ff9",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230404-151441.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P84DT72920S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230404-151441",
"name":
"RWTautowait2-20230404-151441",
"schemaKey":
"Session",
"startDate":
"2023-04-04T15:15:20-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:23.760946+02:00",
"id":
"urn:uuid:42441879-785e-4c95-8b46-f0a7bc1a6009",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:22.481838+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:26.760960Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230404-151441.nwb",
"size":
1408563
},
{
"asset_id":
"e90bc9bd-d5d1-49bc-b703-a9392ee2fd00",
"blob":
"19bf0b1a-ee38-439a-95ef-9bc22fea92d0",
"created":
"2024-09-13T12:58:28.044576Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:43:56.786650+02:00",
"contentSize":
1456430,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/e90bc9bd-d5d1-49bc-b703-a9392ee2fd00/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/19b/f0b/19bf0b1a-ee38-439a-95ef-9bc22fea92d0"
],
"dateModified":
"2024-09-13T14:58:24.266180+02:00",
"digest":
{
"dandi:dandi-etag":
"709b93af6805ae3d850ea5e918e40f6b-1",
"dandi:sha2-256":
"a65a321472cec51650afcf76afd707234e47f42f27eec663cd90b09617762778"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:e90bc9bd-d5d1-49bc-b703-a9392ee2fd00",
"identifier":
"e90bc9bd-d5d1-49bc-b703-a9392ee2fd00",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230405-145629.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P85DT71852S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230405-145629",
"name":
"RWTautowait2-20230405-145629",
"schemaKey":
"Session",
"startDate":
"2023-04-05T14:57:32-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:24.266174+02:00",
"id":
"urn:uuid:322cce43-d9ab-4f3a-a9cb-bbc24b922548",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:23.018671+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:28.061189Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230405-145629.nwb",
"size":
1456430
},
{
"asset_id":
"bd391932-6325-4bac-9781-3318dae5dc6f",
"blob":
"0a263760-c474-486b-8a21-71731ab0653b",
"created":
"2024-09-13T12:58:27.367345Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:44:06.449204+02:00",
"contentSize":
1417283,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/bd391932-6325-4bac-9781-3318dae5dc6f/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/0a2/637/0a263760-c474-486b-8a21-71731ab0653b"
],
"dateModified":
"2024-09-13T14:58:24.360240+02:00",
"digest":
{
"dandi:dandi-etag":
"ebfe32bf32b37d3e32f0cf905eb6f01c-1",
"dandi:sha2-256":
"cdc4a1cbc0e1cf4126efa5fc23e905119c89d7187114ce3d149367bfddef268e"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:bd391932-6325-4bac-9781-3318dae5dc6f",
"identifier":
"bd391932-6325-4bac-9781-3318dae5dc6f",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230406-150248.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P86DT72195S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230406-150248",
"name":
"RWTautowait2-20230406-150248",
"schemaKey":
"Session",
"startDate":
"2023-04-06T15:03:15-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:24.360227+02:00",
"id":
"urn:uuid:db24feb0-3f3b-409d-8b6b-5cf48d0d6e5a",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:22.423091+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:27.384834Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230406-150248.nwb",
"size":
1417283
},
{
"asset_id":
"3ddaedeb-7b93-432a-b936-a18273669709",
"blob":
"5a3f0021-0276-44e7-8de9-2ddbac04b02e",
"created":
"2024-09-13T12:58:28.116706Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:44:16.392558+02:00",
"contentSize":
1403825,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/3ddaedeb-7b93-432a-b936-a18273669709/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/5a3/f00/5a3f0021-0276-44e7-8de9-2ddbac04b02e"
],
"dateModified":
"2024-09-13T14:58:24.259897+02:00",
"digest":
{
"dandi:dandi-etag":
"7b537ccc42714f2c9e5e70e2c6779c22-1",
"dandi:sha2-256":
"dd902a836dedaba582eaf389dcce552d6a3808c37c6471c335b57444bab1183e"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:3ddaedeb-7b93-432a-b936-a18273669709",
"identifier":
"3ddaedeb-7b93-432a-b936-a18273669709",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230410-153607.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P90DT74307S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230410-153607",
"name":
"RWTautowait2-20230410-153607",
"schemaKey":
"Session",
"startDate":
"2023-04-10T15:38:27-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:24.259882+02:00",
"id":
"urn:uuid:cc306e9f-95af-4860-908d-cf3574e11a95",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:22.926712+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:28.131719Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230410-153607.nwb",
"size":
1403825
},
{
"asset_id":
"49f55b2a-ad6a-4bfc-8a2e-ddb5f1047965",
"blob":
"a2f241df-41ac-4af5-8613-b67fc155dd10",
"created":
"2024-09-13T12:58:31.321217Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:44:25.193940+02:00",
"contentSize":
1338139,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/49f55b2a-ad6a-4bfc-8a2e-ddb5f1047965/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/a2f/241/a2f241df-41ac-4af5-8613-b67fc155dd10"
],
"dateModified":
"2024-09-13T14:58:28.517123+02:00",
"digest":
{
"dandi:dandi-etag":
"a1e7319083c308ee128c4b5d7d10ea5b-1",
"dandi:sha2-256":
"556ee8a46df513c9c1f0285dd7fb446122e3c08fd1a62121ce41c551eaae2686"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:49f55b2a-ad6a-4bfc-8a2e-ddb5f1047965",
"identifier":
"49f55b2a-ad6a-4bfc-8a2e-ddb5f1047965",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230411-153745.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P91DT74313S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230411-153745",
"name":
"RWTautowait2-20230411-153745",
"schemaKey":
"Session",
"startDate":
"2023-04-11T15:38:33-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:28.517107+02:00",
"id":
"urn:uuid:4ef62895-5db6-427f-b46b-05fa7c93a1a0",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:27.191972+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:31.336867Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230411-153745.nwb",
"size":
1338139
},
{
"asset_id":
"52b7fcfb-58ca-4975-95f4-4a26af12a919",
"blob":
"620ae9d4-cbb5-412e-a6b5-c2b64ec737cf",
"created":
"2024-09-13T12:58:33.374284Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:44:33.135878+02:00",
"contentSize":
1219246,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/52b7fcfb-58ca-4975-95f4-4a26af12a919/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/620/ae9/620ae9d4-cbb5-412e-a6b5-c2b64ec737cf"
],
"dateModified":
"2024-09-13T14:58:30.563167+02:00",
"digest":
{
"dandi:dandi-etag":
"4b9af645096c9e73a0bd29bcfd964028-1",
"dandi:sha2-256":
"86c25f1f51711030da553362307637f20dbe073b6b25eea7badc08fa5311a5c9"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:52b7fcfb-58ca-4975-95f4-4a26af12a919",
"identifier":
"52b7fcfb-58ca-4975-95f4-4a26af12a919",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230412-160231.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P92DT75806S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230412-160231",
"name":
"RWTautowait2-20230412-160231",
"schemaKey":
"Session",
"startDate":
"2023-04-12T16:03:26-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:30.563138+02:00",
"id":
"urn:uuid:562c5fda-ff56-4dda-9005-b2345ea31aa8",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:29.304697+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:33.390251Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230412-160231.nwb",
"size":
1219246
},
{
"asset_id":
"0b50804a-e296-489e-86ce-8d55d6157bf8",
"blob":
"3bbc51b7-09c2-441a-a8b9-4d42b768e947",
"created":
"2024-09-13T12:58:33.805581Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:44:42.466939+02:00",
"contentSize":
1358369,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/0b50804a-e296-489e-86ce-8d55d6157bf8/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/3bb/c51/3bbc51b7-09c2-441a-a8b9-4d42b768e947"
],
"dateModified":
"2024-09-13T14:58:30.988013+02:00",
"digest":
{
"dandi:dandi-etag":
"58690664858b74e1107a3b1a457fb04e-1",
"dandi:sha2-256":
"8fd98924e23d3d1ae08c29eee72a61d2cc1f1267c481ec0991ff164c99ca24b5"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:0b50804a-e296-489e-86ce-8d55d6157bf8",
"identifier":
"0b50804a-e296-489e-86ce-8d55d6157bf8",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230413-154803.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P93DT74950S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230413-154803",
"name":
"RWTautowait2-20230413-154803",
"schemaKey":
"Session",
"startDate":
"2023-04-13T15:49:10-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:30.987999+02:00",
"id":
"urn:uuid:819f8473-2183-43fd-aafe-eed176ba9cc7",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:30.132887+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:33.816575Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230413-154803.nwb",
"size":
1358369
},
{
"asset_id":
"68d4c64b-148c-4247-baa2-f0efae4d962c",
"blob":
"9ce9d346-ef26-4cd3-bf74-3dde03958393",
"created":
"2024-09-13T12:58:34.912790Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:44:51.383837+02:00",
"contentSize":
1280336,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/68d4c64b-148c-4247-baa2-f0efae4d962c/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/9ce/9d3/9ce9d346-ef26-4cd3-bf74-3dde03958393"
],
"dateModified":
"2024-09-13T14:58:32.029109+02:00",
"digest":
{
"dandi:dandi-etag":
"8a6525cfc0d4637681fa039ad3d52036-1",
"dandi:sha2-256":
"f6c18c838dfa438e45fe68bd9318694993a53437875143cd16753f42502706dd"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:68d4c64b-148c-4247-baa2-f0efae4d962c",
"identifier":
"68d4c64b-148c-4247-baa2-f0efae4d962c",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230414-154700.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P94DT74851S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230414-154700",
"name":
"RWTautowait2-20230414-154700",
"schemaKey":
"Session",
"startDate":
"2023-04-14T15:47:31-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:32.029098+02:00",
"id":
"urn:uuid:1776b5c2-732c-4506-9747-54d59c108cd1",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:31.081695+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:34.939188Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230414-154700.nwb",
"size":
1280336
},
{
"asset_id":
"534058b8-c32d-4f83-94fd-918a6537a08f",
"blob":
"d06308b8-1a06-4578-92e4-0e626e4c3ab9",
"created":
"2024-09-13T12:58:35.055381Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:45:04.436861+02:00",
"contentSize":
1664535,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/534058b8-c32d-4f83-94fd-918a6537a08f/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/d06/308/d06308b8-1a06-4578-92e4-0e626e4c3ab9"
],
"dateModified":
"2024-09-13T14:58:32.094659+02:00",
"digest":
{
"dandi:dandi-etag":
"6517931662563097ef74326fb6e1f7d2-1",
"dandi:sha2-256":
"d5e1ed801281c319fd4f9e4ed613d0d0d4cd7bdccf196afc4f0d538d66543143"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:534058b8-c32d-4f83-94fd-918a6537a08f",
"identifier":
"534058b8-c32d-4f83-94fd-918a6537a08f",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230417-152205.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P97DT73468S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230417-152205",
"name":
"RWTautowait2-20230417-152205",
"schemaKey":
"Session",
"startDate":
"2023-04-17T15:24:28-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:32.094645+02:00",
"id":
"urn:uuid:b762fd6e-a96d-4349-9bbb-aba8d1563564",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:31.051147+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:35.068918Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230417-152205.nwb",
"size":
1664535
},
{
"asset_id":
"f6b09ad8-86e2-4553-a601-58ab50558a9e",
"blob":
"19b6c93f-f457-4332-85f8-b249f74cb252",
"created":
"2024-09-13T12:58:35.969415Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:45:19.456720+02:00",
"contentSize":
1754982,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/f6b09ad8-86e2-4553-a601-58ab50558a9e/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/19b/6c9/19b6c93f-f457-4332-85f8-b249f74cb252"
],
"dateModified":
"2024-09-13T14:58:33.155520+02:00",
"digest":
{
"dandi:dandi-etag":
"dc0b2b89cd6dd8129baaf9fe861ed8ff-1",
"dandi:sha2-256":
"7266fd34be4f8ea221df76d3f6183f182c269bb991a50507e286d7b38aaeb2fa"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:f6b09ad8-86e2-4553-a601-58ab50558a9e",
"identifier":
"f6b09ad8-86e2-4553-a601-58ab50558a9e",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230418-160628.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P98DT76019S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230418-160628",
"name":
"RWTautowait2-20230418-160628",
"schemaKey":
"Session",
"startDate":
"2023-04-18T16:06:59-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:33.155506+02:00",
"id":
"urn:uuid:7f0f966e-0c2e-455e-aff2-98cff80244a7",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:32.812604+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:35.986832Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230418-160628.nwb",
"size":
1754982
},
{
"asset_id":
"4db5a679-45b4-42ef-90a5-ac2121421365",
"blob":
"6fc6eebb-5812-496b-9083-3d3fc909a4fa",
"created":
"2024-09-13T12:58:42.680520Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:45:27.236476+02:00",
"contentSize":
1133688,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/4db5a679-45b4-42ef-90a5-ac2121421365/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/6fc/6ee/6fc6eebb-5812-496b-9083-3d3fc909a4fa"
],
"dateModified":
"2024-09-13T14:58:35.583177+02:00",
"digest":
{
"dandi:dandi-etag":
"d226127d50c0b7e53c413534ab7b1c2c-1",
"dandi:sha2-256":
"1bb62a4ede67528bc30d68dda07c4afe653ed5929ed6cc782440b1f03f3f485f"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:4db5a679-45b4-42ef-90a5-ac2121421365",
"identifier":
"4db5a679-45b4-42ef-90a5-ac2121421365",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230419-132054.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P99DT66477S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230419-132054",
"name":
"RWTautowait2-20230419-132054",
"schemaKey":
"Session",
"startDate":
"2023-04-19T13:27:57-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:35.583162+02:00",
"id":
"urn:uuid:429a80fa-7d22-4450-960d-1f705961a310",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:34.594949+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:42.701980Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230419-132054.nwb",
"size":
1133688
},
{
"asset_id":
"437c2fe5-e5ab-41a4-abe0-4b439249c57c",
"blob":
"707b22e7-8027-49bb-b5cd-b343872eb169",
"created":
"2024-09-13T12:58:43.337672Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:45:36.388913+02:00",
"contentSize":
1268411,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/437c2fe5-e5ab-41a4-abe0-4b439249c57c/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/707/b22/707b22e7-8027-49bb-b5cd-b343872eb169"
],
"dateModified":
"2024-09-13T14:58:36.322742+02:00",
"digest":
{
"dandi:dandi-etag":
"7d2826200fac26fe7091225d9d81df22-1",
"dandi:sha2-256":
"ebfac9620fc2a19e33baf4e7117673e8be0d455cc314d972d738b76ab0968d78"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:437c2fe5-e5ab-41a4-abe0-4b439249c57c",
"identifier":
"437c2fe5-e5ab-41a4-abe0-4b439249c57c",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230420-140203.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P100DT68710S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230420-140203",
"name":
"RWTautowait2-20230420-140203",
"schemaKey":
"Session",
"startDate":
"2023-04-20T14:05:10-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:36.322725+02:00",
"id":
"urn:uuid:3a8782c5-9376-4a82-a8f8-9e8db53e819d",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:35.204988+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:43.372116Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230420-140203.nwb",
"size":
1268411
},
{
"asset_id":
"0281d320-c118-4049-82b7-d3c6a26efb68",
"blob":
"d2349a4f-be6d-42e4-9fdb-f16f8bec07c9",
"created":
"2024-09-13T12:58:41.059833Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:45:45.832155+02:00",
"contentSize":
1240636,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/0281d320-c118-4049-82b7-d3c6a26efb68/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/d23/49a/d2349a4f-be6d-42e4-9fdb-f16f8bec07c9"
],
"dateModified":
"2024-09-13T14:58:38.167968+02:00",
"digest":
{
"dandi:dandi-etag":
"529318bc18b4f4d7384be43feee87534-1",
"dandi:sha2-256":
"11564d07fe0324498795dcac7edb3fdb889275f08d95e0e04d7303c1e562537a"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:0281d320-c118-4049-82b7-d3c6a26efb68",
"identifier":
"0281d320-c118-4049-82b7-d3c6a26efb68",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230421-140201.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P101DT68602S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230421-140201",
"name":
"RWTautowait2-20230421-140201",
"schemaKey":
"Session",
"startDate":
"2023-04-21T14:03:22-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:38.167952+02:00",
"id":
"urn:uuid:c7f283ed-f7bd-49c9-8c8c-ef8c3059abd9",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:37.435691+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:41.078267Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230421-140201.nwb",
"size":
1240636
},
{
"asset_id":
"7e3fa3d3-1d83-4082-8fec-6b75241ede49",
"blob":
"2a101e80-d78c-4132-bac5-b8e36ebb21fc",
"created":
"2024-09-13T12:58:44.700900Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:45:58.711118+02:00",
"contentSize":
1506698,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/7e3fa3d3-1d83-4082-8fec-6b75241ede49/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/2a1/01e/2a101e80-d78c-4132-bac5-b8e36ebb21fc"
],
"dateModified":
"2024-09-13T14:58:38.491935+02:00",
"digest":
{
"dandi:dandi-etag":
"15f726956dfe441fbab036d10e1673f7-1",
"dandi:sha2-256":
"1906eea89d3749a01ecf1c27dd42740aff3e79d0b7dc3b81974e4941b3b62298"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:7e3fa3d3-1d83-4082-8fec-6b75241ede49",
"identifier":
"7e3fa3d3-1d83-4082-8fec-6b75241ede49",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230424-131712.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P104DT65948S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230424-131712",
"name":
"RWTautowait2-20230424-131712",
"schemaKey":
"Session",
"startDate":
"2023-04-24T13:19:08-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:38.491918+02:00",
"id":
"urn:uuid:6970fc58-58cb-4bad-a51f-34cc6d55bc6e",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:37.753310+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:44.716342Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230424-131712.nwb",
"size":
1506698
},
{
"asset_id":
"515cfbf5-8d94-429b-be34-921d8e0b1d37",
"blob":
"1f1aa3fc-0be9-46c7-b9d2-bd23c5011a26",
"created":
"2024-09-13T12:58:43.251044Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:46:12.334106+02:00",
"contentSize":
1594251,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/515cfbf5-8d94-429b-be34-921d8e0b1d37/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/1f1/aa3/1f1aa3fc-0be9-46c7-b9d2-bd23c5011a26"
],
"dateModified":
"2024-09-13T14:58:38.720428+02:00",
"digest":
{
"dandi:dandi-etag":
"c809928e3b0eb26600707e63c68617cf-1",
"dandi:sha2-256":
"df03e656f97cacfd0e31948489f02e06e657af71af0d5ef37bf2a5a027034471"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:515cfbf5-8d94-429b-be34-921d8e0b1d37",
"identifier":
"515cfbf5-8d94-429b-be34-921d8e0b1d37",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230425-141122.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P105DT69160S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230425-141122",
"name":
"RWTautowait2-20230425-141122",
"schemaKey":
"Session",
"startDate":
"2023-04-25T14:12:40-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:38.720418+02:00",
"id":
"urn:uuid:7d6b3053-fc3b-4208-8306-785c0a8b4890",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:38.092414+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:43.266314Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230425-141122.nwb",
"size":
1594251
},
{
"asset_id":
"80199f04-e10d-4123-9854-f1099c0bd32b",
"blob":
"f44ad90a-8d5f-4559-8531-0a06e91598ea",
"created":
"2024-09-13T12:58:45.135260Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:46:22.863910+02:00",
"contentSize":
1377793,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/80199f04-e10d-4123-9854-f1099c0bd32b/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/f44/ad9/f44ad90a-8d5f-4559-8531-0a06e91598ea"
],
"dateModified":
"2024-09-13T14:58:42.451221+02:00",
"digest":
{
"dandi:dandi-etag":
"754192cbfa2f12d162f5d637c71345e9-1",
"dandi:sha2-256":
"744885b9d1c58438372e82a2f0adfdffe123efa1d51c8dd1e67abb032f156125"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:80199f04-e10d-4123-9854-f1099c0bd32b",
"identifier":
"80199f04-e10d-4123-9854-f1099c0bd32b",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230426-131955.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P106DT66081S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230426-131955",
"name":
"RWTautowait2-20230426-131955",
"schemaKey":
"Session",
"startDate":
"2023-04-26T13:21:21-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:42.451209+02:00",
"id":
"urn:uuid:d608756e-8ef4-444a-a1b7-27bde1ee10e6",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:42.073321+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:45.148356Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230426-131955.nwb",
"size":
1377793
},
{
"asset_id":
"2e58d8f4-dbd6-46b8-8f72-8f4a143dc193",
"blob":
"2551e1c6-25ac-453e-8c2c-45546532dab0",
"created":
"2024-09-13T12:58:48.413200Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:46:33.623711+02:00",
"contentSize":
1415113,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/2e58d8f4-dbd6-46b8-8f72-8f4a143dc193/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/255/1e1/2551e1c6-25ac-453e-8c2c-45546532dab0"
],
"dateModified":
"2024-09-13T14:58:45.703430+02:00",
"digest":
{
"dandi:dandi-etag":
"a8eb460ab46fd9a89e491d0f3b5539db-1",
"dandi:sha2-256":
"d995db6fb162c1dd63c193d0872a3ac9b4f972c8f9a72f84f62bec24177a7788"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:2e58d8f4-dbd6-46b8-8f72-8f4a143dc193",
"identifier":
"2e58d8f4-dbd6-46b8-8f72-8f4a143dc193",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230427-140713.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P107DT68932S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230427-140713",
"name":
"RWTautowait2-20230427-140713",
"schemaKey":
"Session",
"startDate":
"2023-04-27T14:08:52-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:45.703418+02:00",
"id":
"urn:uuid:05fafd38-1d50-428c-8c57-74fc1d373be2",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:44.202047+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:48.426712Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230427-140713.nwb",
"size":
1415113
},
{
"asset_id":
"5f69a9c7-254a-481d-bced-aa302ba59c5a",
"blob":
"5c6f083d-62b9-4323-baa8-d7e232dfc9c8",
"created":
"2024-09-13T12:58:49.354620Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:46:42.268404+02:00",
"contentSize":
1215880,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/5f69a9c7-254a-481d-bced-aa302ba59c5a/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/5c6/f08/5c6f083d-62b9-4323-baa8-d7e232dfc9c8"
],
"dateModified":
"2024-09-13T14:58:46.598852+02:00",
"digest":
{
"dandi:dandi-etag":
"e0fae0a07b4dba8c12dc792c32ef3d09-1",
"dandi:sha2-256":
"324bf9a0003a52ce10da1a11bbb416ab8c2427b9df734a9bd1ce7db428a02630"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:5f69a9c7-254a-481d-bced-aa302ba59c5a",
"identifier":
"5f69a9c7-254a-481d-bced-aa302ba59c5a",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230428-134631.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P108DT67678S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230428-134631",
"name":
"RWTautowait2-20230428-134631",
"schemaKey":
"Session",
"startDate":
"2023-04-28T13:47:58-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:46.598842+02:00",
"id":
"urn:uuid:6e01011e-27a3-44b8-8d82-bcbd37c578c4",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:45.513831+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:49.367136Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230428-134631.nwb",
"size":
1215880
},
{
"asset_id":
"cd288712-2919-419d-87ea-bc91962015fc",
"blob":
"5561b0df-6ca1-4ee9-80fa-9cfe8b824374",
"created":
"2024-09-13T12:58:55.249115Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:46:54.535731+02:00",
"contentSize":
1542107,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/cd288712-2919-419d-87ea-bc91962015fc/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/556/1b0/5561b0df-6ca1-4ee9-80fa-9cfe8b824374"
],
"dateModified":
"2024-09-13T14:58:46.528454+02:00",
"digest":
{
"dandi:dandi-etag":
"270904bb96e883fda7b63737e8c35aab-1",
"dandi:sha2-256":
"cbac1384f0e0ef9d28cc7ea3225a4f7ece53e2ee9eab1d2baacf87d49c481696"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:cd288712-2919-419d-87ea-bc91962015fc",
"identifier":
"cd288712-2919-419d-87ea-bc91962015fc",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230501-132258.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P111DT66235S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230501-132258",
"name":
"RWTautowait2-20230501-132258",
"schemaKey":
"Session",
"startDate":
"2023-05-01T13:23:55-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:46.528443+02:00",
"id":
"urn:uuid:70a8c558-da1a-4e90-9245-313307809b25",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:45.377061+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:55.260299Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230501-132258.nwb",
"size":
1542107
},
{
"asset_id":
"813c70a6-c39f-42eb-b07a-b7b1bb3a550d",
"blob":
"65deeeaf-e6a2-424d-b631-d6f553ef9143",
"created":
"2024-09-13T12:58:56.548768Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:47:06.286390+02:00",
"contentSize":
1467203,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/813c70a6-c39f-42eb-b07a-b7b1bb3a550d/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/65d/eee/65deeeaf-e6a2-424d-b631-d6f553ef9143"
],
"dateModified":
"2024-09-13T14:58:48.425361+02:00",
"digest":
{
"dandi:dandi-etag":
"e889a426960892fd50341fadd1f6ca90-1",
"dandi:sha2-256":
"d202549843e27b7c5dbe535185a305ff0375942ee8d85acdc2e6a14a8929f156"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:813c70a6-c39f-42eb-b07a-b7b1bb3a550d",
"identifier":
"813c70a6-c39f-42eb-b07a-b7b1bb3a550d",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230502-142032.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P112DT69709S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230502-142032",
"name":
"RWTautowait2-20230502-142032",
"schemaKey":
"Session",
"startDate":
"2023-05-02T14:21:49-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:48.425347+02:00",
"id":
"urn:uuid:7884d915-3a19-4012-b3de-3c8327b27acf",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:47.743917+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:56.560994Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230502-142032.nwb",
"size":
1467203
},
{
"asset_id":
"14af97ea-9b1a-4158-9d1b-5361122f7fbe",
"blob":
"d6929474-5710-4441-b627-cd11f2db7673",
"created":
"2024-09-13T12:58:56.456472Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:47:17.904288+02:00",
"contentSize":
1441776,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/14af97ea-9b1a-4158-9d1b-5361122f7fbe/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/d69/294/d6929474-5710-4441-b627-cd11f2db7673"
],
"dateModified":
"2024-09-13T14:58:48.364607+02:00",
"digest":
{
"dandi:dandi-etag":
"e7fa2b22e505baaa31415b7073e017f6-1",
"dandi:sha2-256":
"d888b75fc104d50d7f09e07a41fe73fb57904531e0c703dff451f0bb5bfffa29"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:14af97ea-9b1a-4158-9d1b-5361122f7fbe",
"identifier":
"14af97ea-9b1a-4158-9d1b-5361122f7fbe",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230503-131944.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P113DT66054S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230503-131944",
"name":
"RWTautowait2-20230503-131944",
"schemaKey":
"Session",
"startDate":
"2023-05-03T13:20:54-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:48.364597+02:00",
"id":
"urn:uuid:9239b5e6-eb0e-4aed-bc59-a0a17e6b5d3c",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:47.673162+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:56.467984Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230503-131944.nwb",
"size":
1441776
},
{
"asset_id":
"3cfe6976-57f6-4657-bb19-02b99fad65e1",
"blob":
"22099cdb-2f07-407c-8a1b-741c4447a15d",
"created":
"2024-09-13T12:58:53.168747Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:47:30.623606+02:00",
"contentSize":
1562916,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/3cfe6976-57f6-4657-bb19-02b99fad65e1/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/220/99c/22099cdb-2f07-407c-8a1b-741c4447a15d"
],
"dateModified":
"2024-09-13T14:58:50.188693+02:00",
"digest":
{
"dandi:dandi-etag":
"8b03f4fd1ab7a845fef656f5e751083c-1",
"dandi:sha2-256":
"541e1208a3b086278ad47abe87b7d46ce6bbb0f92533aee61eba6fc2399d711d"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:3cfe6976-57f6-4657-bb19-02b99fad65e1",
"identifier":
"3cfe6976-57f6-4657-bb19-02b99fad65e1",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230504-135942.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P114DT68433S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230504-135942",
"name":
"RWTautowait2-20230504-135942",
"schemaKey":
"Session",
"startDate":
"2023-05-04T14:00:33-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:50.188683+02:00",
"id":
"urn:uuid:90b82d41-8382-4b9d-b927-db3c2653b88c",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:49.433638+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:53.181741Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230504-135942.nwb",
"size":
1562916
},
{
"asset_id":
"1d508ae7-1a62-4df8-9b44-1db1ce2de9ec",
"blob":
"f25a05ab-ff49-47da-a761-1d5af19f41d0",
"created":
"2024-09-13T12:58:54.001338Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:47:41.535875+02:00",
"contentSize":
1411786,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/1d508ae7-1a62-4df8-9b44-1db1ce2de9ec/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/f25/a05/f25a05ab-ff49-47da-a761-1d5af19f41d0"
],
"dateModified":
"2024-09-13T14:58:51.077013+02:00",
"digest":
{
"dandi:dandi-etag":
"2f92aa3fd345fd2e58a1a32c4ba85818-1",
"dandi:sha2-256":
"94464a6b07fd0c873518a4215da984734f532a657b68150350e7ac0462093593"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:1d508ae7-1a62-4df8-9b44-1db1ce2de9ec",
"identifier":
"1d508ae7-1a62-4df8-9b44-1db1ce2de9ec",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230508-131557.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P118DT65850S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230508-131557",
"name":
"RWTautowait2-20230508-131557",
"schemaKey":
"Session",
"startDate":
"2023-05-08T13:17:30-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:51.077000+02:00",
"id":
"urn:uuid:411baf69-641a-4d37-978c-225e97c0cd7a",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:50.755434+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:54.014231Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230508-131557.nwb",
"size":
1411786
},
{
"asset_id":
"da25ba92-bd84-41f5-97a2-201ece1891a5",
"blob":
"6f14d27a-13a6-46f1-903c-d1dbba33d232",
"created":
"2024-09-13T12:59:04.693661Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:47:56.321855+02:00",
"contentSize":
1704252,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/da25ba92-bd84-41f5-97a2-201ece1891a5/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/6f1/4d2/6f14d27a-13a6-46f1-903c-d1dbba33d232"
],
"dateModified":
"2024-09-13T14:58:54.872060+02:00",
"digest":
{
"dandi:dandi-etag":
"3ed5307fce483aed06bd20fca2e5ac5d-1",
"dandi:sha2-256":
"522344543085bc5e001211bd7f26f5ecf26686465c63cb0e8e8d48d2a35af1ff"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:da25ba92-bd84-41f5-97a2-201ece1891a5",
"identifier":
"da25ba92-bd84-41f5-97a2-201ece1891a5",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230509-140032.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P119DT68476S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230509-140032",
"name":
"RWTautowait2-20230509-140032",
"schemaKey":
"Session",
"startDate":
"2023-05-09T14:01:16-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:54.872049+02:00",
"id":
"urn:uuid:6e9428ac-c79c-4fe2-88ac-64dced8eb11e",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:54.194207+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:04.705903Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230509-140032.nwb",
"size":
1704252
},
{
"asset_id":
"8fffec6a-ce39-40e4-90c3-5847c92da045",
"blob":
"32bab725-c5b6-4a42-9f5b-9c025c3a57f4",
"created":
"2024-09-13T12:58:59.337398Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:48:08.052180+02:00",
"contentSize":
1454697,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/8fffec6a-ce39-40e4-90c3-5847c92da045/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/32b/ab7/32bab725-c5b6-4a42-9f5b-9c025c3a57f4"
],
"dateModified":
"2024-09-13T14:58:56.420656+02:00",
"digest":
{
"dandi:dandi-etag":
"5a4b147ffba2f22701f6b101c32c9191-1",
"dandi:sha2-256":
"ac2e2381d6a07a01717be0449ca9d7762d5961f35f130f8db055c7682b1b3ca3"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:8fffec6a-ce39-40e4-90c3-5847c92da045",
"identifier":
"8fffec6a-ce39-40e4-90c3-5847c92da045",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230510-133312.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P120DT66888S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230510-133312",
"name":
"RWTautowait2-20230510-133312",
"schemaKey":
"Session",
"startDate":
"2023-05-10T13:34:48-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:56.420644+02:00",
"id":
"urn:uuid:b20ca30b-9e0f-4805-99f3-25385a9986ff",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:55.571761+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:58:59.353111Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230510-133312.nwb",
"size":
1454697
},
{
"asset_id":
"0465b27c-7a1d-453d-bf03-45f20afd6501",
"blob":
"3b0d7362-d4ca-4b4c-b89a-fa8fd6a9097a",
"created":
"2024-09-13T12:59:00.916419Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:48:20.697710+02:00",
"contentSize":
1527790,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/0465b27c-7a1d-453d-bf03-45f20afd6501/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/3b0/d73/3b0d7362-d4ca-4b4c-b89a-fa8fd6a9097a"
],
"dateModified":
"2024-09-13T14:58:57.698756+02:00",
"digest":
{
"dandi:dandi-etag":
"e92dd29c99e821e2e4931923caf0c4b1-1",
"dandi:sha2-256":
"dd6cb91409841cd2e55639118822d1dcf0bd9f500335a0322e88512d0fe9be1a"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:0465b27c-7a1d-453d-bf03-45f20afd6501",
"identifier":
"0465b27c-7a1d-453d-bf03-45f20afd6501",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230511-130740.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P121DT65328S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230511-130740",
"name":
"RWTautowait2-20230511-130740",
"schemaKey":
"Session",
"startDate":
"2023-05-11T13:08:48-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:57.698741+02:00",
"id":
"urn:uuid:a99a8006-26a5-43fb-839b-aebd84898b64",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:56.828986+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:00.927311Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230511-130740.nwb",
"size":
1527790
},
{
"asset_id":
"06faa407-6242-4cbf-b045-99067fb34e16",
"blob":
"40ecfbf8-cf0e-4ad0-a7ce-2f79b1b7ebc3",
"created":
"2024-09-13T12:59:02.094949Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:48:30.120301+02:00",
"contentSize":
1243484,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/06faa407-6242-4cbf-b045-99067fb34e16/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/40e/cfb/40ecfbf8-cf0e-4ad0-a7ce-2f79b1b7ebc3"
],
"dateModified":
"2024-09-13T14:58:59.290879+02:00",
"digest":
{
"dandi:dandi-etag":
"cc9005d5b39a45a6a2df1d82beb6aad5-1",
"dandi:sha2-256":
"715e87b66fe0d1eb2f3f939830c1ffa67f225f08830f7d620120035fe86a53c8"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:06faa407-6242-4cbf-b045-99067fb34e16",
"identifier":
"06faa407-6242-4cbf-b045-99067fb34e16",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230512-132833.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P122DT66562S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230512-132833",
"name":
"RWTautowait2-20230512-132833",
"schemaKey":
"Session",
"startDate":
"2023-05-12T13:29:22-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:59.290873+02:00",
"id":
"urn:uuid:67e9cfec-d810-4dac-b2b7-3230fe4abc1a",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:58.621427+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:02.111779Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230512-132833.nwb",
"size":
1243484
},
{
"asset_id":
"90f1a3c0-d2e1-4f5e-98af-b7fbd0412706",
"blob":
"551f7f01-a217-4b42-81f3-90453f55e429",
"created":
"2024-09-13T12:59:02.177086Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:48:39.138263+02:00",
"contentSize":
1167121,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/90f1a3c0-d2e1-4f5e-98af-b7fbd0412706/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/551/f7f/551f7f01-a217-4b42-81f3-90453f55e429"
],
"dateModified":
"2024-09-13T14:58:59.289374+02:00",
"digest":
{
"dandi:dandi-etag":
"c43695667ad9956b83909f09937a8add-1",
"dandi:sha2-256":
"ce03bd21df6642e8d5a2eb5563ecebec605905aca7cfef5a49bd14a66f3c438c"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:90f1a3c0-d2e1-4f5e-98af-b7fbd0412706",
"identifier":
"90f1a3c0-d2e1-4f5e-98af-b7fbd0412706",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230515-130814.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P125DT65368S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230515-130814",
"name":
"RWTautowait2-20230515-130814",
"schemaKey":
"Session",
"startDate":
"2023-05-15T13:09:28-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:58:59.289363+02:00",
"id":
"urn:uuid:d8ea1f51-0cd6-4f3c-9adf-888108594324",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:58:58.588412+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:02.193034Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230515-130814.nwb",
"size":
1167121
},
{
"asset_id":
"5cba245c-26d4-4ac0-bba7-a446afebae81",
"blob":
"6ab2e728-26c5-4e9a-b80b-25d1cefbcd2e",
"created":
"2024-09-13T12:59:09.194556Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:48:52.849799+02:00",
"contentSize":
1546750,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/5cba245c-26d4-4ac0-bba7-a446afebae81/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/6ab/2e7/6ab2e728-26c5-4e9a-b80b-25d1cefbcd2e"
],
"dateModified":
"2024-09-13T14:59:00.681906+02:00",
"digest":
{
"dandi:dandi-etag":
"32e9a13803503d617c93d84739a0a842-1",
"dandi:sha2-256":
"c162c27fe8a3438f36fb44a76fb7ce8ef25f9f17da8b8ec067e1c737544c6af8"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:5cba245c-26d4-4ac0-bba7-a446afebae81",
"identifier":
"5cba245c-26d4-4ac0-bba7-a446afebae81",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230516-125449.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P126DT64557S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230516-125449",
"name":
"RWTautowait2-20230516-125449",
"schemaKey":
"Session",
"startDate":
"2023-05-16T12:55:57-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:00.681894+02:00",
"id":
"urn:uuid:8f1fb022-a64b-4ff1-ba7c-b36b9da59723",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:00.290000+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:09.210046Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230516-125449.nwb",
"size":
1546750
},
{
"asset_id":
"41fbd525-c12f-4ff7-bc78-e1d0b7851dea",
"blob":
"bb6bf172-d418-4b49-b0cc-10baa88623b8",
"created":
"2024-09-13T12:59:05.583914Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:49:04.638903+02:00",
"contentSize":
1430086,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/41fbd525-c12f-4ff7-bc78-e1d0b7851dea/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/bb6/bf1/bb6bf172-d418-4b49-b0cc-10baa88623b8"
],
"dateModified":
"2024-09-13T14:59:02.706015+02:00",
"digest":
{
"dandi:dandi-etag":
"308afb96da31ea2d62da54d36f89bcb5-1",
"dandi:sha2-256":
"b06022b2a466b1ad49039c1cde16efb6531a5d6b573ea75e5ff956bf03d73781"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:41fbd525-c12f-4ff7-bc78-e1d0b7851dea",
"identifier":
"41fbd525-c12f-4ff7-bc78-e1d0b7851dea",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230517-131233.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P127DT65618S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230517-131233",
"name":
"RWTautowait2-20230517-131233",
"schemaKey":
"Session",
"startDate":
"2023-05-17T13:13:38-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:02.706001+02:00",
"id":
"urn:uuid:038e3244-e453-4ff9-85ea-bc5ab7fc04ba",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:01.913216+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:05.596938Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230517-131233.nwb",
"size":
1430086
},
{
"asset_id":
"f3b7beea-9d3a-4937-bbf8-00f9056345f8",
"blob":
"3a3a3979-4a7a-4048-90db-bb6b34550657",
"created":
"2024-09-13T12:59:12.485357Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:49:15.439328+02:00",
"contentSize":
1381880,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/f3b7beea-9d3a-4937-bbf8-00f9056345f8/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/3a3/a39/3a3a3979-4a7a-4048-90db-bb6b34550657"
],
"dateModified":
"2024-09-13T14:59:04.650123+02:00",
"digest":
{
"dandi:dandi-etag":
"1fafadcb22c9d02674e7320de6844af9-1",
"dandi:sha2-256":
"267de5dc2df687a8c79bcdf85e0637b538cfbcacf51788700fe736f808ab4136"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:f3b7beea-9d3a-4937-bbf8-00f9056345f8",
"identifier":
"f3b7beea-9d3a-4937-bbf8-00f9056345f8",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230518-132402.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P128DT66284S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230518-132402",
"name":
"RWTautowait2-20230518-132402",
"schemaKey":
"Session",
"startDate":
"2023-05-18T13:24:44-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:04.650117+02:00",
"id":
"urn:uuid:993325fc-51ca-42b5-b5dd-2ea4edc533a9",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:03.908099+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:12.510439Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230518-132402.nwb",
"size":
1381880
},
{
"asset_id":
"6bb8fb81-c654-48df-b1c1-3375c9f9cbec",
"blob":
"fc3b1fe0-2048-447f-97be-d14f7680e2d3",
"created":
"2024-09-13T12:59:11.951042Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:49:25.175400+02:00",
"contentSize":
1256031,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/6bb8fb81-c654-48df-b1c1-3375c9f9cbec/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/fc3/b1f/fc3b1fe0-2048-447f-97be-d14f7680e2d3"
],
"dateModified":
"2024-09-13T14:59:04.647354+02:00",
"digest":
{
"dandi:dandi-etag":
"e46f2adbbc4d026fa91ff7561feae84d-1",
"dandi:sha2-256":
"3fe66d88f6c206ce508d41462a5fc10f195845cbf259be3f3e03d13fd8d1361e"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:6bb8fb81-c654-48df-b1c1-3375c9f9cbec",
"identifier":
"6bb8fb81-c654-48df-b1c1-3375c9f9cbec",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230519-130722.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P129DT65318S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230519-130722",
"name":
"RWTautowait2-20230519-130722",
"schemaKey":
"Session",
"startDate":
"2023-05-19T13:08:38-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:04.647345+02:00",
"id":
"urn:uuid:575da5c8-bcd9-4318-864d-386acc20a502",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:03.941218+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:11.964798Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230519-130722.nwb",
"size":
1256031
},
{
"asset_id":
"bd4a93fb-fe01-4a27-b829-1d79fbaed87e",
"blob":
"f477a0e5-8996-45a7-9e01-70359fb88370",
"created":
"2024-09-13T12:59:09.410700Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:49:37.712453+02:00",
"contentSize":
1496511,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/bd4a93fb-fe01-4a27-b829-1d79fbaed87e/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/f47/7a0/f477a0e5-8996-45a7-9e01-70359fb88370"
],
"dateModified":
"2024-09-13T14:59:06.548807+02:00",
"digest":
{
"dandi:dandi-etag":
"c560a954e57fd4ed74ab1f2aa1a17254-1",
"dandi:sha2-256":
"204f8eae6aefd8c2a0c4b4571687ab155a80fd2016ca50a5aca5cb27756232f2"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:bd4a93fb-fe01-4a27-b829-1d79fbaed87e",
"identifier":
"bd4a93fb-fe01-4a27-b829-1d79fbaed87e",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230522-131136.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P132DT65614S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230522-131136",
"name":
"RWTautowait2-20230522-131136",
"schemaKey":
"Session",
"startDate":
"2023-05-22T13:13:34-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:06.548796+02:00",
"id":
"urn:uuid:1a0833d0-e955-4dc6-aace-dbbef5e0591a",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:05.828577+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:09.447849Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230522-131136.nwb",
"size":
1496511
},
{
"asset_id":
"dc2e728e-5ed2-4d1f-8c28-1a206dee558a",
"blob":
"fa70c829-1e5f-4af1-bbb2-da5462530bc0",
"created":
"2024-09-13T12:59:10.057337Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:49:52.835768+02:00",
"contentSize":
1628114,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/dc2e728e-5ed2-4d1f-8c28-1a206dee558a/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/fa7/0c8/fa70c829-1e5f-4af1-bbb2-da5462530bc0"
],
"dateModified":
"2024-09-13T14:59:07.291174+02:00",
"digest":
{
"dandi:dandi-etag":
"24f1506f6c1123c92f326a78df675479-1",
"dandi:sha2-256":
"e9a58923444ad9422e4daf1a03b9b9874b2d6b0b57922d0bad018277ccca5ba5"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:dc2e728e-5ed2-4d1f-8c28-1a206dee558a",
"identifier":
"dc2e728e-5ed2-4d1f-8c28-1a206dee558a",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230523-130714.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P133DT65331S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230523-130714",
"name":
"RWTautowait2-20230523-130714",
"schemaKey":
"Session",
"startDate":
"2023-05-23T13:08:51-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:07.291164+02:00",
"id":
"urn:uuid:fca44309-d9f5-4395-a9e7-ffd33e1728da",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:06.956931+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:10.069544Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230523-130714.nwb",
"size":
1628114
},
{
"asset_id":
"871e8ca0-006d-4397-a569-5fde5b7fa9b1",
"blob":
"b4605de6-e7d1-41d6-8ea9-9d3520561197",
"created":
"2024-09-13T12:59:14.993621Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:50:05.493371+02:00",
"contentSize":
1513042,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/871e8ca0-006d-4397-a569-5fde5b7fa9b1/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/b46/05d/b4605de6-e7d1-41d6-8ea9-9d3520561197"
],
"dateModified":
"2024-09-13T14:59:11.808523+02:00",
"digest":
{
"dandi:dandi-etag":
"2721c0f12bec947b083f1a09d03c77c9-1",
"dandi:sha2-256":
"f523bad2ddd6dfb78dfd459701ee885a39f0e2be2dd0c66681134aba75699463"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:871e8ca0-006d-4397-a569-5fde5b7fa9b1",
"identifier":
"871e8ca0-006d-4397-a569-5fde5b7fa9b1",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230524-132107.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P134DT66105S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230524-132107",
"name":
"RWTautowait2-20230524-132107",
"schemaKey":
"Session",
"startDate":
"2023-05-24T13:21:45-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:11.808512+02:00",
"id":
"urn:uuid:cdc6e92e-12d4-40db-b2b4-08ab6b342184",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:10.787196+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:15.025567Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230524-132107.nwb",
"size":
1513042
},
{
"asset_id":
"83aaae4d-d1bb-4045-bcc6-fab8733262cd",
"blob":
"373ee950-72b3-4dd1-82c0-690f0cd4c755",
"created":
"2024-09-13T12:59:15.291777Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:50:17.761400+02:00",
"contentSize":
1449271,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/83aaae4d-d1bb-4045-bcc6-fab8733262cd/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/373/ee9/373ee950-72b3-4dd1-82c0-690f0cd4c755"
],
"dateModified":
"2024-09-13T14:59:12.344024+02:00",
"digest":
{
"dandi:dandi-etag":
"d7f406a8fd52f3c864a2a8751e8bc5ae-1",
"dandi:sha2-256":
"b1ba94ad5868e9a390946038bab467a49866591296bacbd8d74facd976060fb4"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:83aaae4d-d1bb-4045-bcc6-fab8733262cd",
"identifier":
"83aaae4d-d1bb-4045-bcc6-fab8733262cd",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230525-125212.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P135DT64392S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230525-125212",
"name":
"RWTautowait2-20230525-125212",
"schemaKey":
"Session",
"startDate":
"2023-05-25T12:53:12-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:12.344012+02:00",
"id":
"urn:uuid:8baf14fd-3fe3-4c2b-997f-a44f26fb5e58",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:11.206910+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:15.305908Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230525-125212.nwb",
"size":
1449271
},
{
"asset_id":
"3ebb48fe-1625-4d63-bec0-1f53fd53502f",
"blob":
"ce10379f-17bb-44cf-abe3-d1bcb350767f",
"created":
"2024-09-13T12:59:20.371943Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:50:29.228409+02:00",
"contentSize":
1432475,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/3ebb48fe-1625-4d63-bec0-1f53fd53502f/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/ce1/037/ce10379f-17bb-44cf-abe3-d1bcb350767f"
],
"dateModified":
"2024-09-13T14:59:13.116543+02:00",
"digest":
{
"dandi:dandi-etag":
"7c89d80cc60707acf1a2b1d404ca18fd-1",
"dandi:sha2-256":
"0c6daa78f3aea4e61d5650923fcf33c5d3c627d594658d530f0c78fcd732aba8"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:3ebb48fe-1625-4d63-bec0-1f53fd53502f",
"identifier":
"3ebb48fe-1625-4d63-bec0-1f53fd53502f",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230526-131149.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P136DT65540S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230526-131149",
"name":
"RWTautowait2-20230526-131149",
"schemaKey":
"Session",
"startDate":
"2023-05-26T13:12:20-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:13.116533+02:00",
"id":
"urn:uuid:c919735d-71b1-4ab0-bc68-28c213079c53",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:12.075738+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:20.386143Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230526-131149.nwb",
"size":
1432475
},
{
"asset_id":
"d3f8a58c-4d5e-401a-8f33-f57ffadcdf06",
"blob":
"aec443d6-b47a-4b8c-a0a4-30b26b8e363b",
"created":
"2024-09-13T12:59:17.942362Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:50:42.934786+02:00",
"contentSize":
1556192,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/d3f8a58c-4d5e-401a-8f33-f57ffadcdf06/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/aec/443/aec443d6-b47a-4b8c-a0a4-30b26b8e363b"
],
"dateModified":
"2024-09-13T14:59:14.957798+02:00",
"digest":
{
"dandi:dandi-etag":
"f618adc68aa80f7b65853b0b12ed345a-1",
"dandi:sha2-256":
"aa824d8e02da85825002ed978e1245338bdcc4f005f349a2fa69abef77019ede"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:d3f8a58c-4d5e-401a-8f33-f57ffadcdf06",
"identifier":
"d3f8a58c-4d5e-401a-8f33-f57ffadcdf06",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230530-130729.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P140DT65325S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230530-130729",
"name":
"RWTautowait2-20230530-130729",
"schemaKey":
"Session",
"startDate":
"2023-05-30T13:08:45-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:14.957788+02:00",
"id":
"urn:uuid:03918961-a84d-4247-ac47-49ff35a7b0fd",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:14.200161+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:17.959385Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230530-130729.nwb",
"size":
1556192
},
{
"asset_id":
"5bd4bc5d-7c65-4984-97a0-030b6db0223f",
"blob":
"262fee03-edad-47dc-ab37-d82b0e7390c3",
"created":
"2024-09-13T12:59:18.037976Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:50:54.779402+02:00",
"contentSize":
1417088,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/5bd4bc5d-7c65-4984-97a0-030b6db0223f/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/262/fee/262fee03-edad-47dc-ab37-d82b0e7390c3"
],
"dateModified":
"2024-09-13T14:59:14.961809+02:00",
"digest":
{
"dandi:dandi-etag":
"d121eeca42d579031f9cbe791676991a-1",
"dandi:sha2-256":
"ba4c787aeb6f4c8518d48810eb42bad4d2312dd1fdd48dc1fe8601e9c5d81ecc"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:5bd4bc5d-7c65-4984-97a0-030b6db0223f",
"identifier":
"5bd4bc5d-7c65-4984-97a0-030b6db0223f",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230531-130436.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P141DT65149S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230531-130436",
"name":
"RWTautowait2-20230531-130436",
"schemaKey":
"Session",
"startDate":
"2023-05-31T13:05:49-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:14.961796+02:00",
"id":
"urn:uuid:94569513-6400-413c-aa6d-07c3ab58e410",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:14.267089+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:18.057991Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230531-130436.nwb",
"size":
1417088
},
{
"asset_id":
"e3d65f79-db18-43d8-8724-57569c230696",
"blob":
"c1833cc8-094d-44c2-bf0c-45492e6a9013",
"created":
"2024-09-13T12:59:20.105984Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:51:05.592258+02:00",
"contentSize":
1372077,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/e3d65f79-db18-43d8-8724-57569c230696/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/c18/33c/c1833cc8-094d-44c2-bf0c-45492e6a9013"
],
"dateModified":
"2024-09-13T14:59:17.085898+02:00",
"digest":
{
"dandi:dandi-etag":
"33b7c5ffc15f1fca016122c14541b1f1-1",
"dandi:sha2-256":
"384a1c58b537577d38f0de7fff91ba9f917bfa3efde8c5dcc2bbf57cc74f85e6"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:e3d65f79-db18-43d8-8724-57569c230696",
"identifier":
"e3d65f79-db18-43d8-8724-57569c230696",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230601-133057.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P142DT66692S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230601-133057",
"name":
"RWTautowait2-20230601-133057",
"schemaKey":
"Session",
"startDate":
"2023-06-01T13:31:32-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:17.085889+02:00",
"id":
"urn:uuid:a4a74bc2-5879-47e2-8f66-afbfec175752",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:16.711253+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:20.128976Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230601-133057.nwb",
"size":
1372077
},
{
"asset_id":
"7e38973c-d5cd-4c45-8620-3d991f4edc67",
"blob":
"d6713e50-7aa2-4565-89f5-7955f9e87a8d",
"created":
"2024-09-13T12:59:25.516343Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:51:20.084196+02:00",
"contentSize":
1585339,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/7e38973c-d5cd-4c45-8620-3d991f4edc67/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/d67/13e/d6713e50-7aa2-4565-89f5-7955f9e87a8d"
],
"dateModified":
"2024-09-13T14:59:17.547699+02:00",
"digest":
{
"dandi:dandi-etag":
"fdf89d498b23d7ac631e879ba8512686-1",
"dandi:sha2-256":
"41d969bf55069ca7194a72dd3900595842dbd4b5e4f76b67ac05f9bd74d4856a"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:7e38973c-d5cd-4c45-8620-3d991f4edc67",
"identifier":
"7e38973c-d5cd-4c45-8620-3d991f4edc67",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230605-130332.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P146DT65115S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230605-130332",
"name":
"RWTautowait2-20230605-130332",
"schemaKey":
"Session",
"startDate":
"2023-06-05T13:05:15-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:17.547689+02:00",
"id":
"urn:uuid:86353287-b31a-492d-8f38-71136ba2e01a",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:17.159703+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:25.527433Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230605-130332.nwb",
"size":
1585339
},
{
"asset_id":
"a5e907a6-b53d-4ee1-abd8-635e7e4c0169",
"blob":
"4c0caa55-86f9-4f2b-abd4-507a525602bc",
"created":
"2024-09-13T12:59:28.292328Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:51:31.243044+02:00",
"contentSize":
1361872,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/a5e907a6-b53d-4ee1-abd8-635e7e4c0169/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/4c0/caa/4c0caa55-86f9-4f2b-abd4-507a525602bc"
],
"dateModified":
"2024-09-13T14:59:20.600298+02:00",
"digest":
{
"dandi:dandi-etag":
"2809b244863e93035d298464ac20331a-1",
"dandi:sha2-256":
"44510f949d18349ce8a5f0ce18329904d02134563bbbb1be2f07371e1e450390"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:a5e907a6-b53d-4ee1-abd8-635e7e4c0169",
"identifier":
"a5e907a6-b53d-4ee1-abd8-635e7e4c0169",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230606-130110.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P147DT64934S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230606-130110",
"name":
"RWTautowait2-20230606-130110",
"schemaKey":
"Session",
"startDate":
"2023-06-06T13:02:14-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:20.600285+02:00",
"id":
"urn:uuid:cb124a1c-5463-4db1-9e0a-87806c1bd72a",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:19.634128+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:28.307186Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230606-130110.nwb",
"size":
1361872
},
{
"asset_id":
"7b41103c-3ac7-4799-94f4-5c961e17bc2a",
"blob":
"da45d70b-6129-436c-b928-79754d66ca79",
"created":
"2024-09-13T12:59:23.818922Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:51:43.548618+02:00",
"contentSize":
1465994,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/7b41103c-3ac7-4799-94f4-5c961e17bc2a/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/da4/5d7/da45d70b-6129-436c-b928-79754d66ca79"
],
"dateModified":
"2024-09-13T14:59:20.759018+02:00",
"digest":
{
"dandi:dandi-etag":
"44462747a34310aa4d08f35c833a53e5-1",
"dandi:sha2-256":
"34c3834c52a012518eb0a4ea3058337f9157cc0309f4426b71c040bc9b475000"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:7b41103c-3ac7-4799-94f4-5c961e17bc2a",
"identifier":
"7b41103c-3ac7-4799-94f4-5c961e17bc2a",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230607-130603.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P148DT65189S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230607-130603",
"name":
"RWTautowait2-20230607-130603",
"schemaKey":
"Session",
"startDate":
"2023-06-07T13:06:29-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:20.759006+02:00",
"id":
"urn:uuid:602d7368-69e2-4c3b-a8a7-91c8dd754147",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:19.600293+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:23.840949Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230607-130603.nwb",
"size":
1465994
},
{
"asset_id":
"f6457bfb-3212-40fa-a0b0-be1e15640819",
"blob":
"4fec16ed-0a2c-410d-8491-8a74861d91fd",
"created":
"2024-09-13T12:59:25.695425Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:51:55.378094+02:00",
"contentSize":
1405656,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/f6457bfb-3212-40fa-a0b0-be1e15640819/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/4fe/c16/4fec16ed-0a2c-410d-8491-8a74861d91fd"
],
"dateModified":
"2024-09-13T14:59:22.786727+02:00",
"digest":
{
"dandi:dandi-etag":
"2f84da6e339b2da6784c112e34d6aa86-1",
"dandi:sha2-256":
"7224dca8b3834678176a15411b3b14406fab6ff4bd3c8d17e941870fdbb48756"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:f6457bfb-3212-40fa-a0b0-be1e15640819",
"identifier":
"f6457bfb-3212-40fa-a0b0-be1e15640819",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230608-133626.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P149DT67028S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230608-133626",
"name":
"RWTautowait2-20230608-133626",
"schemaKey":
"Session",
"startDate":
"2023-06-08T13:37:08-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:22.786717+02:00",
"id":
"urn:uuid:21cda90a-8ea8-410e-b6ea-6d47d8439a16",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:22.068991+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:25.717467Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230608-133626.nwb",
"size":
1405656
},
{
"asset_id":
"eb7c2335-0ea7-4695-9c91-6515853c7da9",
"blob":
"594ecbcc-5a44-4cb1-bf5d-cc2fb60a049e",
"created":
"2024-09-13T12:59:25.579898Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:52:06.164403+02:00",
"contentSize":
1273007,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/eb7c2335-0ea7-4695-9c91-6515853c7da9/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/594/ecb/594ecbcc-5a44-4cb1-bf5d-cc2fb60a049e"
],
"dateModified":
"2024-09-13T14:59:22.786931+02:00",
"digest":
{
"dandi:dandi-etag":
"6b23d12905071dca4278075306c1e6cb-1",
"dandi:sha2-256":
"e9f5b9e26181de036334e1cceac2da65f05778867c45b20b370a88f94c868aa5"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:eb7c2335-0ea7-4695-9c91-6515853c7da9",
"identifier":
"eb7c2335-0ea7-4695-9c91-6515853c7da9",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230609-132405.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P150DT66350S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230609-132405",
"name":
"RWTautowait2-20230609-132405",
"schemaKey":
"Session",
"startDate":
"2023-06-09T13:25:50-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:22.786925+02:00",
"id":
"urn:uuid:d1507b56-9858-446a-bd12-03a8e4e8b20d",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:22.098817+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:25.597334Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230609-132405.nwb",
"size":
1273007
},
{
"asset_id":
"0e5ac265-634a-4f19-aff6-c056d8e085b0",
"blob":
"e62d0710-458f-4b0f-9177-eb491af08b17",
"created":
"2024-09-13T12:59:28.083765Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:52:20.101926+02:00",
"contentSize":
1485014,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/0e5ac265-634a-4f19-aff6-c056d8e085b0/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/e62/d07/e62d0710-458f-4b0f-9177-eb491af08b17"
],
"dateModified":
"2024-09-13T14:59:25.197070+02:00",
"digest":
{
"dandi:dandi-etag":
"ab2d86a9759bdefce1d146ea2d4ebe02-1",
"dandi:sha2-256":
"34bec299d3a837dd69f0418af81cb340ff849ec63765bcaf9aa3c04330b0140d"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:0e5ac265-634a-4f19-aff6-c056d8e085b0",
"identifier":
"0e5ac265-634a-4f19-aff6-c056d8e085b0",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230612-133841.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P153DT67185S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230612-133841",
"name":
"RWTautowait2-20230612-133841",
"schemaKey":
"Session",
"startDate":
"2023-06-12T13:39:45-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:25.197059+02:00",
"id":
"urn:uuid:88053c88-80f4-41e8-9685-68514185beef",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:24.782940+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:28.098722Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230612-133841.nwb",
"size":
1485014
},
{
"asset_id":
"3be14da1-40b5-4726-877d-08496a7be9ae",
"blob":
"b00db435-7a55-4cce-8bb3-0f2979468c15",
"created":
"2024-09-13T12:59:31.745058Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:52:32.813540+02:00",
"contentSize":
1439978,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/3be14da1-40b5-4726-877d-08496a7be9ae/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/b00/db4/b00db435-7a55-4cce-8bb3-0f2979468c15"
],
"dateModified":
"2024-09-13T14:59:28.465490+02:00",
"digest":
{
"dandi:dandi-etag":
"9348aa2a7a071a1ac5bebfe300ead4f9-1",
"dandi:sha2-256":
"ebd3c70252aff4550a41cd57b25e8752c00604f40d64f0d446b30b824ba38115"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:3be14da1-40b5-4726-877d-08496a7be9ae",
"identifier":
"3be14da1-40b5-4726-877d-08496a7be9ae",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230613-132046.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P154DT66097S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230613-132046",
"name":
"RWTautowait2-20230613-132046",
"schemaKey":
"Session",
"startDate":
"2023-06-13T13:21:37-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:28.465474+02:00",
"id":
"urn:uuid:77fe490f-d542-4230-8969-99840bf6eafe",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:27.681152+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:31.766780Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230613-132046.nwb",
"size":
1439978
},
{
"asset_id":
"5d07a685-cd2c-4a26-8a53-4e2dd3f9bf86",
"blob":
"b7c15fce-b90c-4298-a16a-891d13f6d88b",
"created":
"2024-09-13T12:59:34.305343Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:52:45.335640+02:00",
"contentSize":
1420383,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/5d07a685-cd2c-4a26-8a53-4e2dd3f9bf86/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/b7c/15f/b7c15fce-b90c-4298-a16a-891d13f6d88b"
],
"dateModified":
"2024-09-13T14:59:29.802314+02:00",
"digest":
{
"dandi:dandi-etag":
"79f235cdbfc8039a21cf90a1cbc6b3f9-1",
"dandi:sha2-256":
"6486f02e3504a3a22d0ba6f88959cf45a3b735a645a3b4dc0960ad0a55e027a0"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:5d07a685-cd2c-4a26-8a53-4e2dd3f9bf86",
"identifier":
"5d07a685-cd2c-4a26-8a53-4e2dd3f9bf86",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230614-130450.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P155DT65137S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230614-130450",
"name":
"RWTautowait2-20230614-130450",
"schemaKey":
"Session",
"startDate":
"2023-06-14T13:05:37-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:29.802300+02:00",
"id":
"urn:uuid:67b302f8-7142-49d4-8e6a-e7e765338477",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:28.080771+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:34.321489Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230614-130450.nwb",
"size":
1420383
},
{
"asset_id":
"2177651d-515c-4cfc-aa4d-d3bf1ab8e241",
"blob":
"01420ad9-9ad7-41aa-a8f0-08a1fff0c6cf",
"created":
"2024-09-13T12:59:32.224091Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:53:00.332999+02:00",
"contentSize":
1605322,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/2177651d-515c-4cfc-aa4d-d3bf1ab8e241/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/014/20a/01420ad9-9ad7-41aa-a8f0-08a1fff0c6cf"
],
"dateModified":
"2024-09-13T14:59:29.304731+02:00",
"digest":
{
"dandi:dandi-etag":
"262ac3d9e7cf53e334ca55690afed2a3-1",
"dandi:sha2-256":
"674d91ec516d63e65855b561933dc5576af71d7c6355390b73395df9d1afe3d4"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:2177651d-515c-4cfc-aa4d-d3bf1ab8e241",
"identifier":
"2177651d-515c-4cfc-aa4d-d3bf1ab8e241",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230615-131538.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P156DT65752S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230615-131538",
"name":
"RWTautowait2-20230615-131538",
"schemaKey":
"Session",
"startDate":
"2023-06-15T13:15:52-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:29.304716+02:00",
"id":
"urn:uuid:83f485ac-b9f3-421d-8804-5887ef8757ed",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:27.953830+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:32.237973Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230615-131538.nwb",
"size":
1605322
},
{
"asset_id":
"069aadd4-1751-4cee-82fd-51cd89e42b97",
"blob":
"ba8aad8a-ce42-44f7-bc70-e43783bb3533",
"created":
"2024-09-13T12:59:34.369807Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:53:11.553991+02:00",
"contentSize":
1341639,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/069aadd4-1751-4cee-82fd-51cd89e42b97/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/ba8/aad/ba8aad8a-ce42-44f7-bc70-e43783bb3533"
],
"dateModified":
"2024-09-13T14:59:31.371267+02:00",
"digest":
{
"dandi:dandi-etag":
"79fcc591dcf960dfd22491fc5afd5c41-1",
"dandi:sha2-256":
"ed0e3589a24dacac9c77ae64e04b8e590ac5eebd00c6f007265b3201d57d3f40"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:069aadd4-1751-4cee-82fd-51cd89e42b97",
"identifier":
"069aadd4-1751-4cee-82fd-51cd89e42b97",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230616-133738.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P157DT67128S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230616-133738",
"name":
"RWTautowait2-20230616-133738",
"schemaKey":
"Session",
"startDate":
"2023-06-16T13:38:48-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:31.371259+02:00",
"id":
"urn:uuid:d381891e-4969-49bb-aed8-df23734ee867",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:30.635230+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:34.383662Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230616-133738.nwb",
"size":
1341639
},
{
"asset_id":
"f9227e56-4b87-4414-90e9-3ca546f0bf75",
"blob":
"464053fe-88dc-4103-a2de-63ac8ff7ba99",
"created":
"2024-09-13T12:59:39.221971Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:53:26.140315+02:00",
"contentSize":
1525601,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/f9227e56-4b87-4414-90e9-3ca546f0bf75/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/464/053/464053fe-88dc-4103-a2de-63ac8ff7ba99"
],
"dateModified":
"2024-09-13T14:59:31.369895+02:00",
"digest":
{
"dandi:dandi-etag":
"cc99fe8ebb3034487238b0c4bb6907db-1",
"dandi:sha2-256":
"81cc43bbde063483a9078df77d687bf2bca83e3893d483e530c312bff246db26"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:f9227e56-4b87-4414-90e9-3ca546f0bf75",
"identifier":
"f9227e56-4b87-4414-90e9-3ca546f0bf75",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230620-132104.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P161DT66156S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230620-132104",
"name":
"RWTautowait2-20230620-132104",
"schemaKey":
"Session",
"startDate":
"2023-06-20T13:22:36-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:31.369882+02:00",
"id":
"urn:uuid:b1e813ff-4f0d-4982-95d4-2dd386c5a886",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:30.670616+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:39.239508Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230620-132104.nwb",
"size":
1525601
},
{
"asset_id":
"a52d8a64-e1c9-42a0-b0e2-b122059db784",
"blob":
"6608137a-5b75-40d4-a021-0e74a15d2117",
"created":
"2024-09-13T12:59:36.440341Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:53:40.209386+02:00",
"contentSize":
1530529,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/a52d8a64-e1c9-42a0-b0e2-b122059db784/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/660/813/6608137a-5b75-40d4-a021-0e74a15d2117"
],
"dateModified":
"2024-09-13T14:59:33.541330+02:00",
"digest":
{
"dandi:dandi-etag":
"18a4ce21e555c369ad8ac4363da85b99-1",
"dandi:sha2-256":
"57019ee37697325103d42a66882b14f6d4889d625a2ef022cc7c0714a3963a91"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:a52d8a64-e1c9-42a0-b0e2-b122059db784",
"identifier":
"a52d8a64-e1c9-42a0-b0e2-b122059db784",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230621-132356.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P162DT66374S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230621-132356",
"name":
"RWTautowait2-20230621-132356",
"schemaKey":
"Session",
"startDate":
"2023-06-21T13:26:14-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:33.541318+02:00",
"id":
"urn:uuid:692b31b6-92c9-4f9a-9ddf-ab2a2760eb3b",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:33.011295+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:36.453522Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230621-132356.nwb",
"size":
1530529
},
{
"asset_id":
"4df2a246-2f1c-4770-af48-1c7466d0db5e",
"blob":
"8f8d81d2-9c36-4540-b562-a777af729272",
"created":
"2024-09-13T12:59:37.687923Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:53:55.805439+02:00",
"contentSize":
1624548,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/4df2a246-2f1c-4770-af48-1c7466d0db5e/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/8f8/d81/8f8d81d2-9c36-4540-b562-a777af729272"
],
"dateModified":
"2024-09-13T14:59:33.905036+02:00",
"digest":
{
"dandi:dandi-etag":
"a6b91e59e9df8133483330c7d474f7ac-1",
"dandi:sha2-256":
"6c9c47b19a3b100c41a075a50cc0181f5691d69f82b4d71b8a2aa9d3c992881e"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:4df2a246-2f1c-4770-af48-1c7466d0db5e",
"identifier":
"4df2a246-2f1c-4770-af48-1c7466d0db5e",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230622-133500.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P163DT66964S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230622-133500",
"name":
"RWTautowait2-20230622-133500",
"schemaKey":
"Session",
"startDate":
"2023-06-22T13:36:04-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:33.905021+02:00",
"id":
"urn:uuid:13140b24-6d0d-44a4-8059-972471daa7b8",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:33.526559+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:37.700930Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230622-133500.nwb",
"size":
1624548
},
{
"asset_id":
"a29bb892-b2f8-4e46-ac73-f5175afd8bf6",
"blob":
"e10915c1-0e72-457f-b4f8-8db0b2598d38",
"created":
"2024-09-13T12:59:39.881517Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:54:05.931783+02:00",
"contentSize":
1248883,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/a29bb892-b2f8-4e46-ac73-f5175afd8bf6/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/e10/915/e10915c1-0e72-457f-b4f8-8db0b2598d38"
],
"dateModified":
"2024-09-13T14:59:36.959825+02:00",
"digest":
{
"dandi:dandi-etag":
"c523ac326a5186fec87f3ca6c6bb304f-1",
"dandi:sha2-256":
"e5bbd3cc3008eb142608507f7fbb47af5147c50307bcec30c7d31a537bf83c96"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:a29bb892-b2f8-4e46-ac73-f5175afd8bf6",
"identifier":
"a29bb892-b2f8-4e46-ac73-f5175afd8bf6",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230623-130805.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P164DT65352S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230623-130805",
"name":
"RWTautowait2-20230623-130805",
"schemaKey":
"Session",
"startDate":
"2023-06-23T13:09:12-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:36.959816+02:00",
"id":
"urn:uuid:3696aa1b-7853-4b75-8a4f-15b3138fd54c",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:36.026187+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:39.893915Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230623-130805.nwb",
"size":
1248883
},
{
"asset_id":
"d3d72d03-8604-41e9-9a87-449f0cb90575",
"blob":
"0768f49e-d482-4824-b341-bf83a7f1be94",
"created":
"2024-09-13T12:59:39.820760Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:54:20.274213+02:00",
"contentSize":
1531504,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/d3d72d03-8604-41e9-9a87-449f0cb90575/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/076/8f4/0768f49e-d482-4824-b341-bf83a7f1be94"
],
"dateModified":
"2024-09-13T14:59:36.926048+02:00",
"digest":
{
"dandi:dandi-etag":
"9233b9a6c071c9be7ff61febac9e2e91-1",
"dandi:sha2-256":
"fe0bfa0f34c0ee6e12396ba3eea238357ff5fa460ef1cd8c7a06d9ef18e400b9"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:d3d72d03-8604-41e9-9a87-449f0cb90575",
"identifier":
"d3d72d03-8604-41e9-9a87-449f0cb90575",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230626-130900.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P167DT65397S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230626-130900",
"name":
"RWTautowait2-20230626-130900",
"schemaKey":
"Session",
"startDate":
"2023-06-26T13:09:57-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:36.926032+02:00",
"id":
"urn:uuid:b31b66f0-bdfc-41ae-9f03-bfc58938e2ca",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:35.976682+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:39.831670Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230626-130900.nwb",
"size":
1531504
},
{
"asset_id":
"4de156ae-b754-4512-9e2a-174433d8cf3d",
"blob":
"8468deea-d289-43dd-a36b-dec2625117a8",
"created":
"2024-09-13T12:59:41.600356Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:54:32.037193+02:00",
"contentSize":
1400848,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/4de156ae-b754-4512-9e2a-174433d8cf3d/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/846/8de/8468deea-d289-43dd-a36b-dec2625117a8"
],
"dateModified":
"2024-09-13T14:59:38.580925+02:00",
"digest":
{
"dandi:dandi-etag":
"ab7e709c569034284fc6be80b876c6a9-1",
"dandi:sha2-256":
"b9a228f2567a3ab0446790c6dea05a856df000394f48c47f61aab2feee298bfa"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:4de156ae-b754-4512-9e2a-174433d8cf3d",
"identifier":
"4de156ae-b754-4512-9e2a-174433d8cf3d",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230627-131514.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P168DT65801S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230627-131514",
"name":
"RWTautowait2-20230627-131514",
"schemaKey":
"Session",
"startDate":
"2023-06-27T13:16:41-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:38.580908+02:00",
"id":
"urn:uuid:b3117cbc-0f17-4a1c-9cff-18bf7eb236b0",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:37.858990+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:41.615112Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230627-131514.nwb",
"size":
1400848
},
{
"asset_id":
"c73d71b2-50da-4ce9-9e4e-50339b5a2c9d",
"blob":
"52261463-72ef-47b1-a229-eeffe39f243d",
"created":
"2024-09-13T12:59:43.453840Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:54:46.379266+02:00",
"contentSize":
1607181,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/c73d71b2-50da-4ce9-9e4e-50339b5a2c9d/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/522/614/52261463-72ef-47b1-a229-eeffe39f243d"
],
"dateModified":
"2024-09-13T14:59:40.367699+02:00",
"digest":
{
"dandi:dandi-etag":
"bb7964231affbb89905555323710f95b-1",
"dandi:sha2-256":
"ae9fca1c9f063761c670f37e3b9a8b138ff8bcbc1859d6c785effa7f57dc818f"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:c73d71b2-50da-4ce9-9e4e-50339b5a2c9d",
"identifier":
"c73d71b2-50da-4ce9-9e4e-50339b5a2c9d",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230628-130740.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P169DT65308S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230628-130740",
"name":
"RWTautowait2-20230628-130740",
"schemaKey":
"Session",
"startDate":
"2023-06-28T13:08:28-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:40.367686+02:00",
"id":
"urn:uuid:91b063a0-7faa-4909-9006-9349750039ee",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:39.207394+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:43.479712Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230628-130740.nwb",
"size":
1607181
},
{
"asset_id":
"a30a8835-aecb-4ba9-b734-e5210180e109",
"blob":
"7d0e38ab-3356-4f22-825a-568fef2d1a62",
"created":
"2024-09-13T12:59:45.886056Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:55:01.120920+02:00",
"contentSize":
1632472,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/a30a8835-aecb-4ba9-b734-e5210180e109/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/7d0/e38/7d0e38ab-3356-4f22-825a-568fef2d1a62"
],
"dateModified":
"2024-09-13T14:59:42.737669+02:00",
"digest":
{
"dandi:dandi-etag":
"55827dc5fa8b2a3a63938a18a07b070f-1",
"dandi:sha2-256":
"febc910d6dbe6f39e68f6b78c1fb6481d72fdb45f63a80b07ce7e750acbf9dc9"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:a30a8835-aecb-4ba9-b734-e5210180e109",
"identifier":
"a30a8835-aecb-4ba9-b734-e5210180e109",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230629-133605.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P170DT66985S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230629-133605",
"name":
"RWTautowait2-20230629-133605",
"schemaKey":
"Session",
"startDate":
"2023-06-29T13:36:25-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:42.737651+02:00",
"id":
"urn:uuid:d2ec58d3-e142-43f6-a770-093b6f9974a9",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:41.273828+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:45.907954Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230629-133605.nwb",
"size":
1632472
},
{
"asset_id":
"0e75a76e-f8b7-4146-9da8-4fd83baacd4a",
"blob":
"98cb6a71-8bfe-4f54-804d-9df66daf5cc0",
"created":
"2024-09-13T12:59:52.770096Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:55:17.891178+02:00",
"contentSize":
1709587,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/0e75a76e-f8b7-4146-9da8-4fd83baacd4a/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/98c/b6a/98cb6a71-8bfe-4f54-804d-9df66daf5cc0"
],
"dateModified":
"2024-09-13T14:59:43.978992+02:00",
"digest":
{
"dandi:dandi-etag":
"68079e081ec3af8fb0009a7dcdd62619-1",
"dandi:sha2-256":
"203eb90a20059ff94472722835e6bdc648b577a374445c1bfcd17e1149cc2a32"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:0e75a76e-f8b7-4146-9da8-4fd83baacd4a",
"identifier":
"0e75a76e-f8b7-4146-9da8-4fd83baacd4a",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230705-130506.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P176DT65179S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230705-130506",
"name":
"RWTautowait2-20230705-130506",
"schemaKey":
"Session",
"startDate":
"2023-07-05T13:06:19-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:43.978979+02:00",
"id":
"urn:uuid:3712b5d4-aa25-4c14-aadb-30382b24fda1",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:42.606666+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:52.783820Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230705-130506.nwb",
"size":
1709587
},
{
"asset_id":
"a09b4464-744c-491b-89f7-1df8be6a504c",
"blob":
"f1586dcc-8633-49d0-b569-02aad01eba1a",
"created":
"2024-09-13T12:59:47.267870Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:55:32.605513+02:00",
"contentSize":
1559789,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/a09b4464-744c-491b-89f7-1df8be6a504c/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/f15/86d/f1586dcc-8633-49d0-b569-02aad01eba1a"
],
"dateModified":
"2024-09-13T14:59:43.981852+02:00",
"digest":
{
"dandi:dandi-etag":
"a6d0d6fccbf7d96edb6f8b3bc43de33a-1",
"dandi:sha2-256":
"5e4756243f97236ee59128ab3dca82495823f5d8f86258e41d98d7eaa7e1a7ba"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:a09b4464-744c-491b-89f7-1df8be6a504c",
"identifier":
"a09b4464-744c-491b-89f7-1df8be6a504c",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230706-131005.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P177DT65470S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230706-131005",
"name":
"RWTautowait2-20230706-131005",
"schemaKey":
"Session",
"startDate":
"2023-07-06T13:11:10-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:43.981843+02:00",
"id":
"urn:uuid:059b70c7-1e5a-4f3f-a90c-e2a43f78296b",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:42.529966+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:47.300603Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230706-131005.nwb",
"size":
1559789
},
{
"asset_id":
"9cdfddba-b5f2-4d11-a958-9740b50fa98c",
"blob":
"ff033a51-d12d-4a01-af5f-775572800feb",
"created":
"2024-09-13T12:59:47.777070Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:55:47.903573+02:00",
"contentSize":
1575228,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/9cdfddba-b5f2-4d11-a958-9740b50fa98c/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/ff0/33a/ff033a51-d12d-4a01-af5f-775572800feb"
],
"dateModified":
"2024-09-13T14:59:44.734975+02:00",
"digest":
{
"dandi:dandi-etag":
"c2182926a5491e186a9737ecc75e8666-1",
"dandi:sha2-256":
"4f65b349d603e4daa6d9319b8c1567c6cdb5f1c8ce1a3f17ce5911ce55d4959c"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:9cdfddba-b5f2-4d11-a958-9740b50fa98c",
"identifier":
"9cdfddba-b5f2-4d11-a958-9740b50fa98c",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230707-130341.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P178DT65105S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230707-130341",
"name":
"RWTautowait2-20230707-130341",
"schemaKey":
"Session",
"startDate":
"2023-07-07T13:05:05-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:44.734965+02:00",
"id":
"urn:uuid:3a575094-8920-414e-af4a-d3bd29d55d46",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:43.782047+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:47.802747Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230707-130341.nwb",
"size":
1575228
},
{
"asset_id":
"0c6a1e1b-8612-42c9-a5ac-eecae778fb01",
"blob":
"961f9ea9-3f47-460d-8fd3-98ff5aedb22f",
"created":
"2024-09-13T12:59:53.744310Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:56:01.476770+02:00",
"contentSize":
1494798,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/0c6a1e1b-8612-42c9-a5ac-eecae778fb01/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/961/f9e/961f9ea9-3f47-460d-8fd3-98ff5aedb22f"
],
"dateModified":
"2024-09-13T14:59:45.676569+02:00",
"digest":
{
"dandi:dandi-etag":
"8fe15de54e4fb8995082dc8637b22c72-1",
"dandi:sha2-256":
"99d1fc5d697aec972f1b3206b1a3c11e0281714f9ab291034bc4a589c1f7aff1"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:0c6a1e1b-8612-42c9-a5ac-eecae778fb01",
"identifier":
"0c6a1e1b-8612-42c9-a5ac-eecae778fb01",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230710-130257.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P181DT65067S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230710-130257",
"name":
"RWTautowait2-20230710-130257",
"schemaKey":
"Session",
"startDate":
"2023-07-10T13:04:27-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:45.676553+02:00",
"id":
"urn:uuid:81189ce5-0a85-4729-8d63-eb0447b28b70",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:45.325862+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:53.790253Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230710-130257.nwb",
"size":
1494798
},
{
"asset_id":
"66aa10c8-3c4e-4d4a-8098-0e21d1686102",
"blob":
"79ae443a-a709-4911-82f2-94b02c3843c8",
"created":
"2024-09-13T12:59:49.917914Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:56:12.549484+02:00",
"contentSize":
1319281,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/66aa10c8-3c4e-4d4a-8098-0e21d1686102/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/79a/e44/79ae443a-a709-4911-82f2-94b02c3843c8"
],
"dateModified":
"2024-09-13T14:59:47.301481+02:00",
"digest":
{
"dandi:dandi-etag":
"6f12a0dbfe1372ad6295a1927c8b5ae6-1",
"dandi:sha2-256":
"9b5d2bd6da35fc1d138ca10be17a6b07d7642da56a0b69d8dca3fa99bc9c7712"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:66aa10c8-3c4e-4d4a-8098-0e21d1686102",
"identifier":
"66aa10c8-3c4e-4d4a-8098-0e21d1686102",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230711-125942.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P182DT64828S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230711-125942",
"name":
"RWTautowait2-20230711-125942",
"schemaKey":
"Session",
"startDate":
"2023-07-11T13:00:28-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:47.301470+02:00",
"id":
"urn:uuid:4308e0c0-c8b2-401b-9552-e41a1c7678b1",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:46.890457+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:49.949534Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230711-125942.nwb",
"size":
1319281
},
{
"asset_id":
"fc99ccc4-8f2f-4fa1-bfc2-a2d445753ec6",
"blob":
"4c6737b9-4614-4601-9481-7d291081542d",
"created":
"2024-09-13T12:59:52.031027Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:56:26.132638+02:00",
"contentSize":
1514674,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/fc99ccc4-8f2f-4fa1-bfc2-a2d445753ec6/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/4c6/737/4c6737b9-4614-4601-9481-7d291081542d"
],
"dateModified":
"2024-09-13T14:59:49.087678+02:00",
"digest":
{
"dandi:dandi-etag":
"322f80b23062f5b53af34904c6a5fc7b-1",
"dandi:sha2-256":
"a24c71b022340e1ec9760929f3f3e63de571561f9f49e9165c962b13fcd0eaab"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:fc99ccc4-8f2f-4fa1-bfc2-a2d445753ec6",
"identifier":
"fc99ccc4-8f2f-4fa1-bfc2-a2d445753ec6",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230712-130207.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P183DT64995S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230712-130207",
"name":
"RWTautowait2-20230712-130207",
"schemaKey":
"Session",
"startDate":
"2023-07-12T13:03:15-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:49.087667+02:00",
"id":
"urn:uuid:2dddca28-21cb-4b25-875e-fa8e823ff8e7",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:48.556027+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:52.056826Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230712-130207.nwb",
"size":
1514674
},
{
"asset_id":
"d7d49686-1da5-4341-a306-a333d85ed9b0",
"blob":
"3322d22f-4509-4b1c-958a-e5c69b387b00",
"created":
"2024-09-13T12:59:52.272177Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:56:40.085101+02:00",
"contentSize":
1547260,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/d7d49686-1da5-4341-a306-a333d85ed9b0/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/332/2d2/3322d22f-4509-4b1c-958a-e5c69b387b00"
],
"dateModified":
"2024-09-13T14:59:49.423929+02:00",
"digest":
{
"dandi:dandi-etag":
"cae98c7d0b32c5c343f8c38163b884b5-1",
"dandi:sha2-256":
"e1a1adeb329e704a3b17f486e7bbdf801bd768b386f56d16b90952c8b1acb381"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:d7d49686-1da5-4341-a306-a333d85ed9b0",
"identifier":
"d7d49686-1da5-4341-a306-a333d85ed9b0",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230713-133557.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P184DT67000S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230713-133557",
"name":
"RWTautowait2-20230713-133557",
"schemaKey":
"Session",
"startDate":
"2023-07-13T13:36:40-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:49.423918+02:00",
"id":
"urn:uuid:5523f2a0-9de8-4536-9c76-f46752aab489",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:49.002857+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:52.284764Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230713-133557.nwb",
"size":
1547260
},
{
"asset_id":
"29a70209-9159-49b0-a252-90c6f32c26e4",
"blob":
"7c218aab-46dc-4f36-ad33-939bc8481352",
"created":
"2024-09-13T12:59:54.162170Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:56:51.385967+02:00",
"contentSize":
1355920,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/29a70209-9159-49b0-a252-90c6f32c26e4/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/7c2/18a/7c218aab-46dc-4f36-ad33-939bc8481352"
],
"dateModified":
"2024-09-13T14:59:51.297899+02:00",
"digest":
{
"dandi:dandi-etag":
"d6ab51a15bb073c0bca7831a4b3af979-1",
"dandi:sha2-256":
"15217b13e7eeda64f76c0a10abebf3aee0c29f0041683c7881d2d56483dc946a"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:29a70209-9159-49b0-a252-90c6f32c26e4",
"identifier":
"29a70209-9159-49b0-a252-90c6f32c26e4",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230714-130112.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P185DT64936S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230714-130112",
"name":
"RWTautowait2-20230714-130112",
"schemaKey":
"Session",
"startDate":
"2023-07-14T13:02:16-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:51.297886+02:00",
"id":
"urn:uuid:1a118979-5bf3-4579-a00c-62db34ccabf1",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:50.940233+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:54.176178Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230714-130112.nwb",
"size":
1355920
},
{
"asset_id":
"c6d8ac93-2c1c-4f5f-9245-d969817cf14e",
"blob":
"2126521c-eaae-4e23-a791-970050e2c932",
"created":
"2024-09-13T12:59:59.193683Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:57:05.576762+02:00",
"contentSize":
1536424,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/c6d8ac93-2c1c-4f5f-9245-d969817cf14e/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/212/652/2126521c-eaae-4e23-a791-970050e2c932"
],
"dateModified":
"2024-09-13T14:59:55.605315+02:00",
"digest":
{
"dandi:dandi-etag":
"dacf02e22ae4d71077e7508fd6d2de46-1",
"dandi:sha2-256":
"8953a03e7f74e54420dadfa4af1c9a33c0d0da6e6910b5bd3486c834ebf7a7b5"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:c6d8ac93-2c1c-4f5f-9245-d969817cf14e",
"identifier":
"c6d8ac93-2c1c-4f5f-9245-d969817cf14e",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230717-130145.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P188DT64987S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230717-130145",
"name":
"RWTautowait2-20230717-130145",
"schemaKey":
"Session",
"startDate":
"2023-07-17T13:03:07-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:55.605299+02:00",
"id":
"urn:uuid:ec80c635-7211-47c4-bfed-177f875282a9",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:54.175952+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:59.246116Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230717-130145.nwb",
"size":
1536424
},
{
"asset_id":
"787768c6-84c8-4abf-b876-9d4fbe3aa0ef",
"blob":
"1af3e209-abed-4ddb-b029-8232bd859549",
"created":
"2024-09-13T12:59:58.300400Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:57:20.937670+02:00",
"contentSize":
1631385,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/787768c6-84c8-4abf-b876-9d4fbe3aa0ef/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/1af/3e2/1af3e209-abed-4ddb-b029-8232bd859549"
],
"dateModified":
"2024-09-13T14:59:55.408531+02:00",
"digest":
{
"dandi:dandi-etag":
"1df3a5bec1c6707ee076891011f2b88c-1",
"dandi:sha2-256":
"d4040793c5b907393032482296214a8bab49609ef8e50cb2c02668b78b958710"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:787768c6-84c8-4abf-b876-9d4fbe3aa0ef",
"identifier":
"787768c6-84c8-4abf-b876-9d4fbe3aa0ef",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230718-130751.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P189DT65309S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230718-130751",
"name":
"RWTautowait2-20230718-130751",
"schemaKey":
"Session",
"startDate":
"2023-07-18T13:08:29-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:55.408517+02:00",
"id":
"urn:uuid:220eced3-3231-4100-bd05-d247f49bf78b",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:54.054358+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T12:59:58.357956Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230718-130751.nwb",
"size":
1631385
},
{
"asset_id":
"4de13240-6e86-4cfc-bc5c-c356f5feb060",
"blob":
"229965e6-e97d-478e-b0b5-0b6c0d99d029",
"created":
"2024-09-13T13:00:05.258882Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:57:36.423550+02:00",
"contentSize":
1596914,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/4de13240-6e86-4cfc-bc5c-c356f5feb060/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/229/965/229965e6-e97d-478e-b0b5-0b6c0d99d029"
],
"dateModified":
"2024-09-13T14:59:56.346884+02:00",
"digest":
{
"dandi:dandi-etag":
"856163586ecb4480133ab18eca3628f3-1",
"dandi:sha2-256":
"c1a8e552c1d4d8bdcdcf5a7ee9dbb02564302cbae37da2a31be8c98fc255cc93"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:4de13240-6e86-4cfc-bc5c-c356f5feb060",
"identifier":
"4de13240-6e86-4cfc-bc5c-c356f5feb060",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230719-130124.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P190DT64913S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230719-130124",
"name":
"RWTautowait2-20230719-130124",
"schemaKey":
"Session",
"startDate":
"2023-07-19T13:01:53-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:56.346874+02:00",
"id":
"urn:uuid:3fc6b1a6-d6b6-4cd5-a15f-0e1867d0a2ee",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:55.205285+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T13:00:05.285565Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230719-130124.nwb",
"size":
1596914
},
{
"asset_id":
"bc258cff-543a-414c-a55b-2ddb57872909",
"blob":
"3544e00a-2370-4240-8229-96bce8b69fd2",
"created":
"2024-09-13T13:00:00.643200Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:57:47.176625+02:00",
"contentSize":
1278557,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/bc258cff-543a-414c-a55b-2ddb57872909/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/354/4e0/3544e00a-2370-4240-8229-96bce8b69fd2"
],
"dateModified":
"2024-09-13T14:59:57.773241+02:00",
"digest":
{
"dandi:dandi-etag":
"c4a8974189a7bbbc509feaf5b160dc90-1",
"dandi:sha2-256":
"565c7e8beb67f556f76589dc0b72cb434de008df38e0ffdcc22d1f766296c418"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:bc258cff-543a-414c-a55b-2ddb57872909",
"identifier":
"bc258cff-543a-414c-a55b-2ddb57872909",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230720-130816.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P191DT65357S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230720-130816",
"name":
"RWTautowait2-20230720-130816",
"schemaKey":
"Session",
"startDate":
"2023-07-20T13:09:17-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:57.773231+02:00",
"id":
"urn:uuid:d0e7d25b-a34e-4144-891d-e73f71c7a4e2",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:57.343043+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T13:00:00.669837Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230720-130816.nwb",
"size":
1278557
},
{
"asset_id":
"d1b9c239-0b9f-41c6-b5bb-3a7e4de637e7",
"blob":
"8620e50d-b87b-4f73-b5e9-5157288ca4e7",
"created":
"2024-09-13T13:00:00.730186Z",
"metadata":
{
"@context":
"https://raw.githubusercontent.com/dandi/schema/master/releases/0.6.8/context.json",
"access":
[
{
"schemaKey":
"AccessRequirements",
"status":
"dandi:OpenAccess"
}
],
"approach":
[],
"blobDateModified":
"2024-09-13T14:57:59.865116+02:00",
"contentSize":
1490565,
"contentUrl":
[
"https://api.dandiarchive.org/api/assets/d1b9c239-0b9f-41c6-b5bb-3a7e4de637e7/download/",
"https://dandiarchive.s3.amazonaws.com/blobs/862/0e5/8620e50d-b87b-4f73-b5e9-5157288ca4e7"
],
"dateModified":
"2024-09-13T14:59:57.401372+02:00",
"digest":
{
"dandi:dandi-etag":
"f0c7083bf808f21be401a645cd369cf9-1",
"dandi:sha2-256":
"a575c2af5527aa07e27ce905e468fcd8a4af6b5c408ef0bcd3006bdfc28464c1"
},
"encodingFormat":
"application/x-nwb",
"id":
"dandiasset:d1b9c239-0b9f-41c6-b5bb-3a7e4de637e7",
"identifier":
"d1b9c239-0b9f-41c6-b5bb-3a7e4de637e7",
"keywords":
[
"decision making",
"reinforcement learning",
"hidden state inference"
],
"measurementTechnique":
[],
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230721-130627.nwb",
"relatedResource":
[
{
"identifier":
"https://doi.org/10.1038/s41467-023-43250-x",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
},
{
"identifier":
"https://doi.org/10.5281/zenodo.10031483",
"relation":
"dcite:IsDescribedBy",
"schemaKey":
"Resource"
}
],
"schemaKey":
"Asset",
"schemaVersion":
"0.6.8",
"variableMeasured":
[],
"wasAttributedTo":
[
{
"age":
{
"schemaKey":
"PropertyValue",
"unitText":
"ISO-8601 duration",
"value":
"P192DT65217S",
"valueReference":
{
"schemaKey":
"PropertyValue",
"value":
"dandi:BirthReference"
}
},
"identifier":
"J076",
"schemaKey":
"Participant",
"sex":
{
"identifier":
"http://purl.obolibrary.org/obo/PATO_0000384",
"name":
"Male",
"schemaKey":
"SexType"
},
"species":
{
"identifier":
"http://purl.obolibrary.org/obo/NCBITaxon_10116",
"name":
"Rattus norvegicus - Norway rat",
"schemaKey":
"SpeciesType"
}
}
],
"wasGeneratedBy":
[
{
"description":
"We developed a temporal wagering task for rats, in which they were offered one of several water rewards on each trial, the volume of which (5, 10, 20, 40, 80μL) was indicated by a tone. The reward was assigned randomly to one of two ports, indicated by an LED. The rat could wait for an unpredictable delay to obtain the reward, or at any time could terminate the trial by poking in the other port (opt-out). Wait times were defined as how long rats waited before opting out. Trial initiation times were defined as the time from opting out or consuming reward to initiating a new trial. Reward delays were drawn from an exponential distribution, and on 15–25 percent of trials, rewards were withheld to force rats to opt-out, providing a continuous behavioral readout of subjective value. We used a high-throughput facility to train 291 rats using computerized, semi-automated procedures. The task contained latent structure; rats experienced blocks of 40 completed trials (hidden states) in which they were presented with low (5, 10, or 20μL) or high (20, 40, or 80μL) rewards. These were interleaved with “mixed\" blocks which offered all rewards. 20μL was present in all blocks, so comparing behavior on trials offering this reward revealed contextual effects (i.e., effects of hidden states). The hidden states differed in their average reward and therefore in their opportunity costs, or what the rat might miss out on by continuing to wait.",
"identifier":
"RWTautowait2-20230721-130627",
"name":
"RWTautowait2-20230721-130627",
"schemaKey":
"Session",
"startDate":
"2023-07-21T13:06:57-04:00"
},
{
"description":
"Metadata generated by DANDI cli",
"endDate":
"2024-09-13T14:59:57.401352+02:00",
"id":
"urn:uuid:3c91fb17-20d5-469d-a66b-2107d013b2f6",
"name":
"Metadata generation",
"schemaKey":
"Activity",
"startDate":
"2024-09-13T14:59:56.907186+02:00",
"wasAssociatedWith":
[
{
"identifier":
"RRID:SCR_019009",
"name":
"DANDI Command Line Interface",
"schemaKey":
"Software",
"url":
"https://github.com/dandi/dandi-cli",
"version":
"0.63.1"
}
]
}
]
},
"modified":
"2024-09-13T13:00:00.750485Z",
"path":
"sub-J076/sub-J076_ses-RWTautowait2-20230721-130627.nwb",
"size":
1490565
}
]